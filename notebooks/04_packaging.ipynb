{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disturbed-division",
   "metadata": {},
   "source": [
    "# Packaging Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-pasta",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "- Package the given model using Torch Model Archive\n",
    "- Write a custom handler to support pre processing and post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-funeral",
   "metadata": {},
   "source": [
    "## Working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-tourist",
   "metadata": {},
   "source": [
    "orignal model and the traced model we saved from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased  distilbert-base-uncased__trace\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../artifacts/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-usage",
   "metadata": {},
   "source": [
    "directory contains tokenizer/ vocab / pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-spirit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t    setup_config.json\t     tokenizer_config.json\r\n",
      "index_to_name.json  special_tokens_map.json  training_args.bin\r\n",
      "pytorch_model.bin   tokenizer.json\t     vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../artifacts/model/distilbert-base-uncased "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sporting-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_to_name.json  special_tokens_map.json  traced_model.pt\r\n",
      "model_store\t    tokenizer.json\t     vocab.txt\r\n",
      "setup_config.json   tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../artifacts/model/distilbert-base-uncased__trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-liver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "instructional-electric",
   "metadata": {},
   "source": [
    "## Torch Model Archiver\n",
    "\n",
    "TorchServe required the model and its dependant artifacts to be packaged in a single file. \n",
    "\n",
    "[torch-model-archiver](https://pypi.org/project/torch-model-archiver/) is a python package that can package the artifacts to a mar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outdoor-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torch-model-archiver [-h] --model-name MODEL_NAME\n",
      "                            [--serialized-file SERIALIZED_FILE]\n",
      "                            [--model-file MODEL_FILE] --handler HANDLER\n",
      "                            [--extra-files EXTRA_FILES]\n",
      "                            [--runtime {python,python2,python3}]\n",
      "                            [--export-path EXPORT_PATH]\n",
      "                            [--archive-format {tgz,no-archive,default}] [-f]\n",
      "                            -v VERSION [-r REQUIREMENTS_FILE]\n",
      "\n",
      "Torch Model Archiver Tool\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model-name MODEL_NAME\n",
      "                        Exported model name. Exported file will be named as\n",
      "                        model-name.mar and saved in current working directory if no --export-path is\n",
      "                        specified, else it will be saved under the export path\n",
      "  --serialized-file SERIALIZED_FILE\n",
      "                        Path to .pt or .pth file containing state_dict in case of eager mode\n",
      "                        or an executable ScriptModule in case of TorchScript.\n",
      "  --model-file MODEL_FILE\n",
      "                        Path to python file containing model architecture.\n",
      "                        This parameter is mandatory for eager mode models.\n",
      "                        The model architecture file must contain only one\n",
      "                        class definition extended from torch.nn.modules.\n",
      "  --handler HANDLER     TorchServe's default handler name\n",
      "                         or Handler path to handle custom inference logic.\n",
      "  --extra-files EXTRA_FILES\n",
      "                        Comma separated path to extra dependency files.\n",
      "  --runtime {python,python2,python3}\n",
      "                        The runtime specifies which language to run your inference code on.\n",
      "                        The default runtime is \"python\".\n",
      "  --export-path EXPORT_PATH\n",
      "                        Path where the exported .mar file will be saved. This is an optional\n",
      "                        parameter. If --export-path is not specified, the file will be saved in the\n",
      "                        current working directory. \n",
      "  --archive-format {tgz,no-archive,default}\n",
      "                        The format in which the model artifacts are archived.\n",
      "                        \"tgz\": This creates the model-archive in <model-name>.tar.gz format.\n",
      "                        If platform hosting TorchServe requires model-artifacts to be in \".tar.gz\"\n",
      "                        use this option.\n",
      "                        \"no-archive\": This option creates an non-archived version of model artifacts\n",
      "                        at \"export-path/{model-name}\" location. As a result of this choice, \n",
      "                        MANIFEST file will be created at \"export-path/{model-name}\" location\n",
      "                        without archiving these model files\n",
      "                        \"default\": This creates the model-archive in <model-name>.mar format.\n",
      "                        This is the default archiving format. Models archived in this format\n",
      "                        will be readily hostable on native TorchServe.\n",
      "  -f, --force           When the -f or --force flag is specified, an existing .mar file with same\n",
      "                        name as that provided in --model-name in the path specified by --export-path\n",
      "                        will overwritten\n",
      "  -v VERSION, --version VERSION\n",
      "                        Model's version\n",
      "  -r REQUIREMENTS_FILE, --requirements-file REQUIREMENTS_FILE\n",
      "                        Path to a requirements.txt containing model specific python dependency\n",
      "                         packages.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "torch-model-archiver --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-nicholas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "insured-paradise",
   "metadata": {},
   "source": [
    "package the model artifact and actual handler code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "searching-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tutorials/personal/pydata_bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Overwriting artifacts/model/distilbert-base-uncased__trace/model_store/pt_classifier.mar ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ..\n",
    "pwd\n",
    "\n",
    "ARTIFACT_BASE_DIR=\"artifacts/model/distilbert-base-uncased__trace\"\n",
    "\n",
    "MODEL_NAME=\"pt_classifier\"\n",
    "MODEL_VERSION=\"1.0\"\n",
    "MODEL_STORE=\"${ARTIFACT_BASE_DIR}/model_store\"\n",
    "MODEL_SERIALIZED_FILE=\"${ARTIFACT_BASE_DIR}/traced_model.pt\"\n",
    "\n",
    "TOKENIZER_FILES=\"${ARTIFACT_BASE_DIR}/tokenizer_config.json,${ARTIFACT_BASE_DIR}/special_tokens_map.json,${ARTIFACT_BASE_DIR}/vocab.txt,${ARTIFACT_BASE_DIR}/tokenizer.json\"\n",
    "MODEL_EXTRA_FILES=\"${ARTIFACT_BASE_DIR}/index_to_name.json,${ARTIFACT_BASE_DIR}/setup_config.json,${TOKENIZER_FILES}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mkdir -p $MODEL_STORE\n",
    "\n",
    "torch-model-archiver --model-name ${MODEL_NAME} \\\n",
    "--version ${MODEL_VERSION} \\\n",
    "--serialized-file ${MODEL_SERIALIZED_FILE} \\\n",
    "--export-path ${MODEL_STORE} \\\n",
    "--extra-files ${MODEL_EXTRA_FILES} \\\n",
    "--handler ./serving/handler.py \\\n",
    "--force\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "yellow-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../serving/handler.py\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from abc import ABC\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import numpy as np\n",
    "from ts.metrics.dimension import Dimension\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import transformers\n",
    "\n",
    "   \n",
    "from abc import ABC\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "from ts.utils.util import map_class_to_label\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Transformers version %s\",transformers.__version__)\n",
    "\n",
    "class CustomHandler(BaseHandler, ABC):\n",
    "    \"\"\"\n",
    "    Transformers handler class for sequence classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "\n",
    "        \n",
    "        self.manifest = ctx.manifest\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda:\" + str(properties.get(\"gpu_id\"))\n",
    "            if torch.cuda.is_available() and properties.get(\"gpu_id\") is not None\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # read configs for the mode, model_name, etc. from setup_config.json\n",
    "        setup_config_path = os.path.join(model_dir, \"setup_config.json\")\n",
    "        if os.path.isfile(setup_config_path):\n",
    "            with open(setup_config_path) as setup_config_file:\n",
    "                self.setup_config = json.load(setup_config_file)\n",
    "        else:\n",
    "            logger.warning(\"Missing the setup_config.json file.\")\n",
    "\n",
    "\n",
    "        # Loading the model and tokenizer from checkpoint and config files based on the user's choice of mode\n",
    "        # further setup config can be added.\n",
    "        if self.setup_config[\"save_mode\"] == \"jit\":\n",
    "            self.model = torch.jit.load(model_pt_path, map_location=self.device)\n",
    "        elif self.setup_config[\"save_mode\"] == \"original\":\n",
    "            self.model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "        else:\n",
    "            logger.warning(\"Missing the checkpoint or state_dict.\")\n",
    "\n",
    "            \n",
    "        \n",
    "        self.top_k = self.setup_config[\"top_k\"]\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir \n",
    "                                                                    , do_lower_case=self.setup_config[\"do_lower_case\"]\n",
    "                                                                    , torchscript=True)\n",
    "\n",
    "      \n",
    "        self.model.eval()\n",
    "\n",
    "        logger.info(\n",
    "            \"Transformer model from path %s loaded successfully\", model_dir\n",
    "        )\n",
    "\n",
    "        # Read the mapping file, index to object name\n",
    "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
    "        \n",
    "        if os.path.isfile(mapping_file_path):\n",
    "            with open(mapping_file_path) as f:\n",
    "                self.mapping = json.load(f)\n",
    "        else:\n",
    "            logger.warning(\"Missing the index_to_name.json file.\")\n",
    "        \n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\"Basic text preprocessing, based on the user's chocie of application mode.\n",
    "        Args:\n",
    "            requests (str): The Input data in the form of text is passed on to the preprocess\n",
    "            function.\n",
    "        Returns:\n",
    "            list : The preprocess function returns a list of Tensor for the size of the word tokens.\n",
    "        \"\"\"\n",
    "        input_ids_batch = None\n",
    "        attention_mask_batch = None\n",
    "        for idx, data in enumerate(requests):\n",
    "            request = data.get(\"data\")\n",
    "            if request is None:\n",
    "                request = data.get(\"body\")\n",
    "            if isinstance(request, (bytes, bytearray)):\n",
    "                request = request.decode('utf-8')\n",
    "\n",
    "            input_text = request['text']\n",
    "            max_length = self.setup_config[\"max_length\"]\n",
    "            logger.info(\"Received text: '%s'\", input_text)\n",
    "\n",
    "            # preprocessing text for sequence_classification and token_classification.\n",
    "            inputs = self.tokenizer.encode_plus(input_text, max_length=int(max_length), pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')\n",
    "            \n",
    "            \n",
    "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "            # making a batch out of the recieved requests\n",
    "            # attention masks are passed for cases where input tokens are padded.\n",
    "            if input_ids.shape is not None:\n",
    "                if input_ids_batch is None:\n",
    "                    input_ids_batch = input_ids\n",
    "                    attention_mask_batch = attention_mask\n",
    "                else:\n",
    "                    input_ids_batch = torch.cat((input_ids_batch, input_ids), 0)\n",
    "                    attention_mask_batch = torch.cat((attention_mask_batch, attention_mask), 0)\n",
    "        return (input_ids_batch, attention_mask_batch)\n",
    "\n",
    "    def inference(self, input_batch):\n",
    "\n",
    "        \n",
    "        input_ids_batch, attention_mask_batch = input_batch\n",
    "        inferences = []\n",
    "        \n",
    "        predictions = self.model(input_ids_batch, attention_mask_batch)\n",
    "        \n",
    "#         ps = torch.nn.functional.softmax(predictions.logits, dim=1)\n",
    "#         probs, classes = torch.topk(ps, self.top_k, dim=1)\n",
    "#         probs = probs.tolist()\n",
    "#         classes = classes.tolist()\n",
    "\n",
    "#         inferences = map_class_to_label(probs, self.mapping, classes)\n",
    "        \n",
    "        for i in range(num_rows):\n",
    "            ps = torch.nn.functional.softmax(predictions[i], dim=1)\n",
    "            probs, classes = torch.topk(ps, self.top_k, dim=1)\n",
    "            probs = probs.tolist()\n",
    "            classes = classes.tolist()\n",
    "        \n",
    "            friendly_labels = map_class_to_label(probs, self.mapping, classes)\n",
    "            inferences.append(friendly_labels)\n",
    "\n",
    "\n",
    "        return inferences\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "\n",
    "        return inference_output\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def handle(self, data, context):\n",
    "\n",
    "        # It can be used for pre or post processing if needed as additional request\n",
    "        # information is available in context\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.context = context\n",
    "        metrics = self.context.metrics\n",
    "        \n",
    "        data_preprocess = self.preprocess(data)\n",
    "        data_inference = self.inference(data_preprocess)\n",
    "        data_postprocess = self.postprocess(data_inference)\n",
    "        \n",
    "        \n",
    "        \n",
    "        stop_time = time.time()\n",
    "        metrics.add_time('HandlerTime', round((stop_time - start_time) * 1000, 2), None, 'ms')\n",
    "        \n",
    "        return data_postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-concept",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "confused-discretion",
   "metadata": {},
   "source": [
    "if you would live to serve through Docker, lets copy the `model_store` artifact relative to the DockerFile folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd .. \n",
    "\n",
    "rm -rf serving/model_store\n",
    "mkdir -p serving/model_store\n",
    "\n",
    "cp artifacts/model/distilbert-base-uncased__trace/model_store/* serving/model_store\n",
    "cp artifacts/model/distilbert-base-uncased__trace/setup_config.json serving/model_store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-hollow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-specialist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-ballot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "funky-summer",
   "metadata": {},
   "source": [
    "## Torchserve\n",
    "\n",
    "> TorchServe is a performant, flexible and easy to use tool for serving PyTorch eager mode and torschripted models.\n",
    "\n",
    "Ref: [TorchServe Docs](https://pytorch.org/serve/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-tokyo",
   "metadata": {},
   "source": [
    "below command starts torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pretty-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "cd ..\n",
    "torchserve --ts-config ./serving/config.properties \\\n",
    "--start --model-store ./serving/model_store --ncs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "missing-champagne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access_log.log\tmodel_log.log  model_metrics.log  ts_log.log  ts_metrics.log\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "endangered-responsibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-25 17:52:13,208 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\r\n",
      "2021-10-25 17:52:13,233 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - model_name: pt_classifier, batchSize: 1\r\n",
      "2021-10-25 17:52:13,676 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Transformers version 4.11.1\r\n",
      "2021-10-25 17:52:35,354 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/84448fbb0cf64f8fa122a52b62531894 loaded successfully\r\n",
      "2021-10-25 17:53:10,874 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:10,874 [WARN ] W-9000-pt_classifier_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\r\n",
      "2021-10-25 17:53:19,419 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:23,218 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:25,753 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:27,878 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'Herbal Tea'\r\n"
     ]
    }
   ],
   "source": [
    "!tail ../logs/model_log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forbidden-marriage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_models=all\r\n",
      "inference_address=http://0.0.0.0:9080\r\n",
      "management_address=http://0.0.0.0:9081\r\n",
      "metrics_address=http://0.0.0.0:9082\r\n",
      "model_store=model_store\r\n",
      "async_logging=true"
     ]
    }
   ],
   "source": [
    "!cat ../serving/config.properties "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-slovak",
   "metadata": {},
   "source": [
    "below command stops torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sixth-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchserve --stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-currency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-premises",
   "metadata": {},
   "source": [
    "List all the models loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "false-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"models\": [\r\n",
      "    {\r\n",
      "      \"modelName\": \"pt_classifier\",\r\n",
      "      \"modelUrl\": \"pt_classifier.mar\"\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl \"http://localhost:9081/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-disposition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "automated-division",
   "metadata": {},
   "source": [
    "get details on the model `pt_classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fancy-rings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"modelName\": \"pt_classifier\",\r\n",
      "    \"modelVersion\": \"1.0\",\r\n",
      "    \"modelUrl\": \"pt_classifier.mar\",\r\n",
      "    \"runtime\": \"python\",\r\n",
      "    \"minWorkers\": 1,\r\n",
      "    \"maxWorkers\": 1,\r\n",
      "    \"batchSize\": 1,\r\n",
      "    \"maxBatchDelay\": 100,\r\n",
      "    \"loadedAtStartup\": true,\r\n",
      "    \"workers\": [\r\n",
      "      {\r\n",
      "        \"id\": \"9000\",\r\n",
      "        \"startTime\": \"2021-10-25T17:52:12.170Z\",\r\n",
      "        \"status\": \"READY\",\r\n",
      "        \"memoryUsage\": 3148378112,\r\n",
      "        \"pid\": 6903,\r\n",
      "        \"gpu\": true,\r\n",
      "        \"gpuUsage\": \"gpuId::0 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::1640 MiB\"\r\n",
      "      }\r\n",
      "    ]\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:9081/models/pt_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-judges",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "premier-scheme",
   "metadata": {},
   "source": [
    "sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norman-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"GROCERY\": 0.9971381425857544,\r\n",
      "    \"HEALTH_PERSONAL_CARE\": 0.002372726332396269,\r\n",
      "    \"PET_SUPPLIES\": 0.00013340394070837647,\r\n",
      "    \"KITCHEN\": 8.001828246051446e-05,\r\n",
      "    \"SHOES\": 3.6940335121471435e-05\r\n",
      "  }\r\n",
      "]\r\n",
      "elasped time (sec):0.039702\r\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST http://localhost:9080/predictions/pt_classifier \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{\"text\":\"herbal tea\",\"request_id\":\"test_id\"}' \\\n",
    "        -w  \"\\nelasped time (sec):%{time_total}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-sherman",
   "metadata": {},
   "source": [
    "sample prediction from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "solid-internship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"GROCERY\": 0.9971381425857544,\n",
      "    \"HEALTH_PERSONAL_CARE\": 0.002372726332396269,\n",
      "    \"PET_SUPPLIES\": 0.00013340394070837647,\n",
      "    \"KITCHEN\": 8.001828246051446e-05,\n",
      "    \"SHOES\": 3.6940335121471435e-05\n",
      "  }\n",
      "]\n",
      "elasped time (sec):0.035042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   265  100   214  100    51   6106   1455 --:--:-- --:--:-- --:--:--  7794\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "curl -X POST http://localhost:9080/predictions/pt_classifier \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d @serving/sample_input.json \\\n",
    "        -w \"\\nelasped time (sec):%{time_total}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-donna",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
