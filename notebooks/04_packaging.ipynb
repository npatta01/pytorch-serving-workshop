{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disturbed-division",
   "metadata": {},
   "source": [
    "# Packaging Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-pasta",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "- Package the given model using Torch Model Archive\n",
    "- Write a custom handler to support pre processing and post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-funeral",
   "metadata": {},
   "source": [
    "## Working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-tourist",
   "metadata": {},
   "source": [
    "orignal model and the traced model we saved from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased  distilbert-base-uncased__trace\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../artifacts/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-usage",
   "metadata": {},
   "source": [
    "directory contains tokenizer/ vocab / pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-spirit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t    setup_config.json\t     tokenizer_config.json\r\n",
      "index_to_name.json  special_tokens_map.json  training_args.bin\r\n",
      "pytorch_model.bin   tokenizer.json\t     vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../artifacts/model/distilbert-base-uncased "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sporting-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_to_name.json  special_tokens_map.json  traced_model.pt\r\n",
      "model_store\t    tokenizer.json\t     vocab.txt\r\n",
      "setup_config.json   tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../artifacts/model/distilbert-base-uncased__trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-liver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "instructional-electric",
   "metadata": {},
   "source": [
    "## Torch Model Archiver\n",
    "\n",
    "TorchServe required the model and its dependant artifacts to be packaged in a single file. \n",
    "\n",
    "[torch-model-archiver](https://pypi.org/project/torch-model-archiver/) is a python package that can package the artifacts to a mar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outdoor-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torch-model-archiver [-h] --model-name MODEL_NAME\n",
      "                            [--serialized-file SERIALIZED_FILE]\n",
      "                            [--model-file MODEL_FILE] --handler HANDLER\n",
      "                            [--extra-files EXTRA_FILES]\n",
      "                            [--runtime {python,python2,python3}]\n",
      "                            [--export-path EXPORT_PATH]\n",
      "                            [--archive-format {tgz,no-archive,default}] [-f]\n",
      "                            -v VERSION [-r REQUIREMENTS_FILE]\n",
      "\n",
      "Torch Model Archiver Tool\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model-name MODEL_NAME\n",
      "                        Exported model name. Exported file will be named as\n",
      "                        model-name.mar and saved in current working directory if no --export-path is\n",
      "                        specified, else it will be saved under the export path\n",
      "  --serialized-file SERIALIZED_FILE\n",
      "                        Path to .pt or .pth file containing state_dict in case of eager mode\n",
      "                        or an executable ScriptModule in case of TorchScript.\n",
      "  --model-file MODEL_FILE\n",
      "                        Path to python file containing model architecture.\n",
      "                        This parameter is mandatory for eager mode models.\n",
      "                        The model architecture file must contain only one\n",
      "                        class definition extended from torch.nn.modules.\n",
      "  --handler HANDLER     TorchServe's default handler name\n",
      "                         or Handler path to handle custom inference logic.\n",
      "  --extra-files EXTRA_FILES\n",
      "                        Comma separated path to extra dependency files.\n",
      "  --runtime {python,python2,python3}\n",
      "                        The runtime specifies which language to run your inference code on.\n",
      "                        The default runtime is \"python\".\n",
      "  --export-path EXPORT_PATH\n",
      "                        Path where the exported .mar file will be saved. This is an optional\n",
      "                        parameter. If --export-path is not specified, the file will be saved in the\n",
      "                        current working directory. \n",
      "  --archive-format {tgz,no-archive,default}\n",
      "                        The format in which the model artifacts are archived.\n",
      "                        \"tgz\": This creates the model-archive in <model-name>.tar.gz format.\n",
      "                        If platform hosting TorchServe requires model-artifacts to be in \".tar.gz\"\n",
      "                        use this option.\n",
      "                        \"no-archive\": This option creates an non-archived version of model artifacts\n",
      "                        at \"export-path/{model-name}\" location. As a result of this choice, \n",
      "                        MANIFEST file will be created at \"export-path/{model-name}\" location\n",
      "                        without archiving these model files\n",
      "                        \"default\": This creates the model-archive in <model-name>.mar format.\n",
      "                        This is the default archiving format. Models archived in this format\n",
      "                        will be readily hostable on native TorchServe.\n",
      "  -f, --force           When the -f or --force flag is specified, an existing .mar file with same\n",
      "                        name as that provided in --model-name in the path specified by --export-path\n",
      "                        will overwritten\n",
      "  -v VERSION, --version VERSION\n",
      "                        Model's version\n",
      "  -r REQUIREMENTS_FILE, --requirements-file REQUIREMENTS_FILE\n",
      "                        Path to a requirements.txt containing model specific python dependency\n",
      "                         packages.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "torch-model-archiver --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-nicholas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "insured-paradise",
   "metadata": {},
   "source": [
    "package the model artifact and actual handler code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "searching-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tutorials/personal/pydata_bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Overwriting artifacts/model/distilbert-base-uncased__trace/model_store/pt_classifier.mar ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ..\n",
    "pwd\n",
    "\n",
    "ARTIFACT_BASE_DIR=\"artifacts/model/distilbert-base-uncased__trace\"\n",
    "\n",
    "MODEL_NAME=\"pt_classifier\"\n",
    "MODEL_VERSION=\"1.0\"\n",
    "MODEL_STORE=\"${ARTIFACT_BASE_DIR}/model_store\"\n",
    "MODEL_SERIALIZED_FILE=\"${ARTIFACT_BASE_DIR}/traced_model.pt\"\n",
    "\n",
    "TOKENIZER_FILES=\"${ARTIFACT_BASE_DIR}/tokenizer_config.json,${ARTIFACT_BASE_DIR}/special_tokens_map.json,${ARTIFACT_BASE_DIR}/vocab.txt,${ARTIFACT_BASE_DIR}/tokenizer.json\"\n",
    "MODEL_EXTRA_FILES=\"${ARTIFACT_BASE_DIR}/index_to_name.json,${ARTIFACT_BASE_DIR}/setup_config.json,${TOKENIZER_FILES}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mkdir -p $MODEL_STORE\n",
    "\n",
    "torch-model-archiver --model-name ${MODEL_NAME} \\\n",
    "--version ${MODEL_VERSION} \\\n",
    "--serialized-file ${MODEL_SERIALIZED_FILE} \\\n",
    "--export-path ${MODEL_STORE} \\\n",
    "--extra-files ${MODEL_EXTRA_FILES} \\\n",
    "--handler ./serving/handler.py \\\n",
    "--force\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "familiar-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../serving/handler.py\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from abc import ABC\n",
    "from collections.abc import Iterable\n",
    "import transformers\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from ts.metrics.dimension import Dimension\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "from ts.utils.util import map_class_to_label\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Transformers version %s\",transformers.__version__)\n",
    "\n",
    "class CustomHandler(BaseHandler, ABC):\n",
    "    \"\"\"\n",
    "    Transformers handler class for sequence classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "\n",
    "        \n",
    "        self.manifest = ctx.manifest\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda:\" + str(properties.get(\"gpu_id\"))\n",
    "            if torch.cuda.is_available() and properties.get(\"gpu_id\") is not None\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # read configs for the mode, model_name, etc. from setup_config.json\n",
    "        setup_config_path = os.path.join(model_dir, \"setup_config.json\")\n",
    "        if os.path.isfile(setup_config_path):\n",
    "            with open(setup_config_path) as setup_config_file:\n",
    "                self.setup_config = json.load(setup_config_file)\n",
    "        else:\n",
    "            logger.warning(\"Missing the setup_config.json file.\")\n",
    "\n",
    "\n",
    "        # Loading the model and tokenizer from checkpoint and config files based on the user's choice of mode\n",
    "        # further setup config can be added.\n",
    "        if self.setup_config[\"save_mode\"] == \"jit\":\n",
    "            self.model = torch.jit.load(model_pt_path, map_location=self.device)\n",
    "        elif self.setup_config[\"save_mode\"] == \"original\":\n",
    "            self.model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "        else:\n",
    "            logger.warning(\"Missing the checkpoint or state_dict.\")\n",
    "\n",
    "            \n",
    "        \n",
    "        self.top_k = self.setup_config[\"top_k\"]\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir \n",
    "                                                                    , do_lower_case=self.setup_config[\"do_lower_case\"]\n",
    "                                                                    , torchscript=True)\n",
    "\n",
    "      \n",
    "        self.model.eval()\n",
    "\n",
    "        logger.info(\n",
    "            \"Transformer model from path %s loaded successfully\", model_dir\n",
    "        )\n",
    "\n",
    "        # Read the mapping file, index to object name\n",
    "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
    "        \n",
    "        if os.path.isfile(mapping_file_path):\n",
    "            with open(mapping_file_path) as f:\n",
    "                self.mapping = json.load(f)\n",
    "        else:\n",
    "            logger.warning(\"Missing the index_to_name.json file.\")\n",
    "        \n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\"Basic text preprocessing, based on the user's chocie of application mode.\n",
    "        Args:\n",
    "            requests (str): The Input data in the form of text is passed on to the preprocess\n",
    "            function.\n",
    "        Returns:\n",
    "            list : The preprocess function returns a list of Tensor for the size of the word tokens.\n",
    "        \"\"\"\n",
    "        input_ids_batch = None\n",
    "        attention_mask_batch = None\n",
    "        for idx, data in enumerate(requests):\n",
    "            request = data.get(\"data\")\n",
    "            if request is None:\n",
    "                request = data.get(\"body\")\n",
    "            if isinstance(request, (bytes, bytearray)):\n",
    "                request = request.decode('utf-8')\n",
    "\n",
    "            input_text = request['text']\n",
    "            max_length = self.setup_config[\"max_length\"]\n",
    "            logger.info(\"Received text: '%s'\", input_text)\n",
    "\n",
    "            # preprocessing text for sequence_classification and token_classification.\n",
    "            inputs = self.tokenizer.encode_plus(input_text, max_length=int(max_length), pad_to_max_length=True, add_special_tokens=True, return_tensors='pt')\n",
    "            \n",
    "            \n",
    "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "            # making a batch out of the recieved requests\n",
    "            # attention masks are passed for cases where input tokens are padded.\n",
    "            if input_ids.shape is not None:\n",
    "                if input_ids_batch is None:\n",
    "                    input_ids_batch = input_ids\n",
    "                    attention_mask_batch = attention_mask\n",
    "                else:\n",
    "                    input_ids_batch = torch.cat((input_ids_batch, input_ids), 0)\n",
    "                    attention_mask_batch = torch.cat((attention_mask_batch, attention_mask), 0)\n",
    "        \n",
    "        input_ids_batch = input_ids_batch.to(self.device)\n",
    "        attention_mask_batch = attention_mask_batch.to(self.device)\n",
    "        \n",
    "        return (input_ids_batch, attention_mask_batch)\n",
    "\n",
    "    def inference(self, input_batch):\n",
    "\n",
    "        \n",
    "        input_ids_batch, attention_mask_batch = input_batch\n",
    "        inferences = []\n",
    "        \n",
    "        predictions = self.model(input_ids_batch, attention_mask_batch)\n",
    "        \n",
    "#         ps = torch.nn.functional.softmax(predictions.logits, dim=1)\n",
    "#         probs, classes = torch.topk(ps, self.top_k, dim=1)\n",
    "#         probs = probs.tolist()\n",
    "#         classes = classes.tolist()\n",
    "\n",
    "#         inferences = map_class_to_label(probs, self.mapping, classes)\n",
    "        \n",
    "        num_rows, num_cols = predictions[0].shape\n",
    "        for i in range(num_rows):\n",
    "            ps = torch.nn.functional.softmax(predictions[i], dim=1)\n",
    "            probs, classes = torch.topk(ps, self.top_k, dim=1)\n",
    "            probs = probs.tolist()\n",
    "            classes = classes.tolist()\n",
    "        \n",
    "            friendly_labels = map_class_to_label(probs, self.mapping, classes)\n",
    "            inferences.append(friendly_labels)\n",
    "\n",
    "\n",
    "        return inferences\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "\n",
    "        return inference_output\n",
    "   \n",
    "    \n",
    "    def handle(self, data, context):\n",
    "\n",
    "        # It can be used for pre or post processing if needed as additional request\n",
    "        # information is available in context\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.context = context\n",
    "        metrics = self.context.metrics\n",
    "        \n",
    "        data_preprocess = self.preprocess(data)\n",
    "        data_inference = self.inference(data_preprocess)\n",
    "        data_postprocess = self.postprocess(data_inference)\n",
    "        \n",
    "        \n",
    "        \n",
    "        stop_time = time.time()\n",
    "        metrics.add_time('HandlerTime', round((stop_time - start_time) * 1000, 2), None, 'ms')\n",
    "        \n",
    "        return data_postprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-concept",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "confused-discretion",
   "metadata": {},
   "source": [
    "if you would live to serve through Docker, lets copy the `model_store` artifact relative to the DockerFile folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd .. \n",
    "\n",
    "rm -rf serving/model_store\n",
    "mkdir -p serving/model_store\n",
    "\n",
    "cp artifacts/model/distilbert-base-uncased__trace/model_store/* serving/model_store\n",
    "cp artifacts/model/distilbert-base-uncased__trace/setup_config.json serving/model_store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-hollow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-specialist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-ballot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "funky-summer",
   "metadata": {},
   "source": [
    "## Torchserve\n",
    "\n",
    "> TorchServe is a performant, flexible and easy to use tool for serving PyTorch eager mode and torschripted models.\n",
    "\n",
    "Ref: [TorchServe Docs](https://pytorch.org/serve/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-tokyo",
   "metadata": {},
   "source": [
    "below command starts torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pretty-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "cd ..\n",
    "torchserve --ts-config ./serving/config.properties \\\n",
    "--start --model-store ./serving/model_store --ncs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "missing-champagne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access_log.log\tmodel_log.log  model_metrics.log  ts_log.log  ts_metrics.log\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "endangered-responsibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-25 17:52:13,676 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Transformers version 4.11.1\r\n",
      "2021-10-25 17:52:35,354 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Transformer model from path /tmp/models/84448fbb0cf64f8fa122a52b62531894 loaded successfully\r\n",
      "2021-10-25 17:53:10,874 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:10,874 [WARN ] W-9000-pt_classifier_1.0-stderr MODEL_LOG - Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\r\n",
      "2021-10-25 17:53:19,419 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:23,218 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:25,753 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-25 17:53:27,878 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'Herbal Tea'\r\n",
      "2021-10-26 01:32:06,505 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'herbal tea'\r\n",
      "2021-10-26 01:32:06,667 [INFO ] W-9000-pt_classifier_1.0-stdout MODEL_LOG - Received text: 'Herbal Tea'\r\n"
     ]
    }
   ],
   "source": [
    "!tail ../logs/model_log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forbidden-marriage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_models=all\r\n",
      "inference_address=http://0.0.0.0:9080\r\n",
      "management_address=http://0.0.0.0:9081\r\n",
      "metrics_address=http://0.0.0.0:9082\r\n",
      "model_store=model_store\r\n",
      "async_logging=true"
     ]
    }
   ],
   "source": [
    "!cat ../serving/config.properties "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-slovak",
   "metadata": {},
   "source": [
    "below command stops torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sixth-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchserve --stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-currency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-premises",
   "metadata": {},
   "source": [
    "List all the models loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "false-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"models\": [\r\n",
      "    {\r\n",
      "      \"modelName\": \"pt_classifier\",\r\n",
      "      \"modelUrl\": \"pt_classifier.mar\"\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl \"http://localhost:9081/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-disposition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "automated-division",
   "metadata": {},
   "source": [
    "get details on the model `pt_classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fancy-rings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"modelName\": \"pt_classifier\",\r\n",
      "    \"modelVersion\": \"1.0\",\r\n",
      "    \"modelUrl\": \"pt_classifier.mar\",\r\n",
      "    \"runtime\": \"python\",\r\n",
      "    \"minWorkers\": 1,\r\n",
      "    \"maxWorkers\": 1,\r\n",
      "    \"batchSize\": 1,\r\n",
      "    \"maxBatchDelay\": 100,\r\n",
      "    \"loadedAtStartup\": true,\r\n",
      "    \"workers\": [\r\n",
      "      {\r\n",
      "        \"id\": \"9000\",\r\n",
      "        \"startTime\": \"2021-10-25T17:52:12.170Z\",\r\n",
      "        \"status\": \"READY\",\r\n",
      "        \"memoryUsage\": 3148378112,\r\n",
      "        \"pid\": 6903,\r\n",
      "        \"gpu\": true,\r\n",
      "        \"gpuUsage\": \"gpuId::0 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::1640 MiB\"\r\n",
      "      }\r\n",
      "    ]\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:9081/models/pt_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-judges",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "premier-scheme",
   "metadata": {},
   "source": [
    "sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "norman-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"GROCERY\": 0.9971381425857544,\r\n",
      "    \"HEALTH_PERSONAL_CARE\": 0.002372726332396269,\r\n",
      "    \"PET_SUPPLIES\": 0.00013340394070837647,\r\n",
      "    \"KITCHEN\": 8.001828246051446e-05,\r\n",
      "    \"SHOES\": 3.6940335121471435e-05\r\n",
      "  }\r\n",
      "]\r\n",
      "elasped time (sec):0.036406\r\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST http://localhost:9080/predictions/pt_classifier \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{\"text\":\"herbal tea\",\"request_id\":\"test_id\"}' \\\n",
    "        -w  \"\\nelasped time (sec):%{time_total}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-sherman",
   "metadata": {},
   "source": [
    "sample prediction from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "solid-internship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"GROCERY\": 0.9971381425857544,\n",
      "    \"HEALTH_PERSONAL_CARE\": 0.002372726332396269,\n",
      "    \"PET_SUPPLIES\": 0.00013340394070837647,\n",
      "    \"KITCHEN\": 8.001828246051446e-05,\n",
      "    \"SHOES\": 3.6940335121471435e-05\n",
      "  }\n",
      "]\n",
      "elasped time (sec):0.034849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r\n",
      "100   265  100   214  100    51   6154   1466 --:--:-- --:--:-- --:--:--  7571\r\n",
      "100   265  100   214  100    51   6140   1463 --:--:-- --:--:-- --:--:--  7571\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "curl -X POST http://localhost:9080/predictions/pt_classifier \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d @serving/sample_input.json \\\n",
    "        -w \"\\nelasped time (sec):%{time_total}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"text\":\"herbal tea\",\"request_id\":\"test_id\"}\n",
    "\n",
    "endpoint = \"http://localhost:9080/predictions/pt_classifier\"\n",
    "\n",
    "res = requests.post(endpoint, json = payload)\n",
    "\n",
    "res.json()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
