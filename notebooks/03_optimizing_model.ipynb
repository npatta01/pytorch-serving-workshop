{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-teach",
   "metadata": {},
   "source": [
    "# Optimizing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-exhibition",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "- Sample code to load artifacts / run inference\n",
    "- Quantizing model\n",
    "- Converting the Bert model with torch script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-hayes",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forbidden-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "import os\n",
    "import json\n",
    "from ts.utils.util  import map_class_to_label\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-pressure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "statistical-absolute",
   "metadata": {},
   "source": [
    "## Load model/tokenizer artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "enclosed-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model= \"distilbert-base-uncased\"\n",
    "model_dir =f'../artifacts/model/{base_model}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-dover",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radio-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir, return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-signal",
   "metadata": {},
   "source": [
    "a utiliy method provided by torchserver requires labels to also be strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loose-result",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'ACCESSORY',\n",
       " '1': 'BOOT',\n",
       " '2': 'CELLULAR_PHONE_CASE',\n",
       " '3': 'CHAIR',\n",
       " '4': 'EARRING',\n",
       " '5': 'FINEEARRING',\n",
       " '6': 'FINENECKLACEBRACELETANKLET',\n",
       " '7': 'FINERING',\n",
       " '8': 'GROCERY',\n",
       " '9': 'HANDBAG',\n",
       " '10': 'HARDWARE_HANDLE',\n",
       " '11': 'HAT',\n",
       " '12': 'HEALTH_PERSONAL_CARE',\n",
       " '13': 'HOME',\n",
       " '14': 'HOME_BED_AND_BATH',\n",
       " '15': 'HOME_FURNITURE_AND_DECOR',\n",
       " '16': 'JANITORIAL_SUPPLY',\n",
       " '17': 'KITCHEN',\n",
       " '18': 'LAMP',\n",
       " '19': 'LIGHT_BULB',\n",
       " '20': 'LIGHT_FIXTURE',\n",
       " '21': 'OFFICE_PRODUCTS',\n",
       " '22': 'OUTDOOR_LIVING',\n",
       " '23': 'PET_SUPPLIES',\n",
       " '24': 'RUG',\n",
       " '25': 'SANDAL',\n",
       " '26': 'SHOES',\n",
       " '27': 'SOFA',\n",
       " '28': 'SPORTING_GOODS',\n",
       " '29': 'TABLE',\n",
       " '30': 'WALL_ART'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_str = {str(key): value for key, value in model.config.id2label.items()}\n",
    "id2label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-december",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-massachusetts",
   "metadata": {},
   "source": [
    "load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controlled-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "                model_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-marine",
   "metadata": {},
   "source": [
    "use gpu if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dental-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-killer",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "julian-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../artifacts/dataset_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "closed-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = datasets.load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ambient-feedback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand', 'item_id', 'item_name', 'main_image_id', 'node'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = set(raw_datasets['test'].column_names ) - set(['text','label'])\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "scientific-worcester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test', 'train', 'valid'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(raw_datasets.column_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-entrance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-sussex",
   "metadata": {},
   "source": [
    "tokenize the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "executed-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0f6b05579142df9fd1a8e417de8469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9d73a0d01542cd94d7c6152856aecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762189a8f9e647a095d0fbd41f72c8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "supported-belarus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention_mask', 'input_ids', 'label', 'text'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['test'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "academic-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of input length of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unusual-reflection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len ( tokenized_datasets['test'][0]['input_ids'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-dressing",
   "metadata": {},
   "source": [
    "create a subset of the test dataset.\n",
    "feel free to use the full dataset if running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "premium-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = tokenized_datasets[\"test\"].num_rows\n",
    "subset = 100\n",
    "\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(42).select(range(subset)) \n",
    "test_dataset.set_format(type='torch' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "linear-musical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-monte",
   "metadata": {},
   "source": [
    "## Predicting on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vanilla-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.97}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.prediction_batch(model,test_dataset,device='cpu' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-newport",
   "metadata": {},
   "source": [
    "## Optimization: Quantizing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-cooperative",
   "metadata": {},
   "source": [
    "Pytorch supports three types of Quantization:\n",
    "1. Dynamic Qunatization\n",
    "2. Static Quantization\n",
    "3. Qunatization Aware Training \n",
    "\n",
    "In this notebook, we look at Dyanmic Quantization.\n",
    "\n",
    "**Dynamic Qunatization**, quantizes the weights . The activations are quantized on the fly.\n",
    "\n",
    "Currently Pytorch doesn't support dynamic quantization on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "scheduled-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=31, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-appliance",
   "metadata": {},
   "source": [
    "In the below code, we quantize all the `Linear` layers to int8 from float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "likely-looking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=31, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "quantized_model_int8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-mountain",
   "metadata": {},
   "source": [
    "How much file storage did we gain ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "floral-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 267.947825\n",
      "Size (MB): 138.734297\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-tournament",
   "metadata": {},
   "source": [
    "The quantized model reduced the file storage by half, which also translates to less memory and faster execution ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-switch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opponent-session",
   "metadata": {},
   "source": [
    "### Benchmark Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-making",
   "metadata": {},
   "source": [
    "Lets measure how much speedup and possible loss in accuracy , we gained from quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "undefined-reviewer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'text'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.select([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "leading-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_model_evaluation(model, dataset=test_dataset,device:str='cpu'):\n",
    "\n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    result = utils.prediction_batch(model , dataset, device=device )\n",
    "    \n",
    "    eval_end_time = time.time()\n",
    "    eval_duration_time_ms =  (eval_end_time - eval_start_time) * 1_000\n",
    "    print(result)\n",
    "    print(\"Evaluate total time (ms): {0:.1f}\".format(eval_duration_time_ms))\n",
    "    \n",
    "def time_model_evaluation_single(model,dataset=test_dataset, device:str='cpu'):\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    tokens_tensor = dataset['input_ids'][0:1]\n",
    "    masks_tensors = dataset['attention_mask'][0:1]\n",
    "\n",
    "\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    masks_tensors = masks_tensors.to(device)\n",
    "    \n",
    "\n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    \n",
    "    res = model(tokens_tensor, masks_tensors)\n",
    "    \n",
    "    eval_end_time = time.time()\n",
    "    eval_duration_time_ms =  (eval_end_time - eval_start_time) * 1_000\n",
    "\n",
    "    #print(\"Evaluate total time (ms): {0:.1f}\".format(eval_duration_time_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-logistics",
   "metadata": {},
   "source": [
    "Evaluate the original FP32 BERT model on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "threaded-prior",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.97}\n",
      "Evaluate total time (ms): 6096.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(model, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "global-shelf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.4 ms ± 1.39 ms per loop (mean ± std. dev. of 3 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 5\n",
    "\n",
    "time_model_evaluation_single(model, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-display",
   "metadata": {},
   "source": [
    "Evaluate the original FP32 BERT model on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "speaking-profile",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.97}\n",
      "Evaluate total time (ms): 12338.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(model, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "combined-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 ms ± 1.32 ms per loop (mean ± std. dev. of 3 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 5\n",
    "time_model_evaluation_single(model, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-offering",
   "metadata": {},
   "source": [
    "Evaluate the qunatized int8 BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "certified-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.97}\n",
      "Evaluate total time (ms): 9957.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(quantized_model_int8, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stable-paste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.3 ms ± 6.21 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 5 -n 10\n",
    "time_model_evaluation_single(quantized_model_int8, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-reggae",
   "metadata": {},
   "source": [
    "dynamtic int8 qunatization is not supported on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expensive-maximum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCould not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. \\nThis could be because the operator doesn't exist for this backend, or was omitted during \\nthe selective/custom build process (if using custom build). \\nIf you are a Facebook employee using PyTorch on mobile, \\nplease visit https://fburl.com/ptmfixes for possible resolutions. \\n'quantized::linear_dynamic' is only available for these backends: [CPU, BackendSelect, Named, \\nADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, \\nAutogradMLC, Tracer, Autocast, Batched, VmapMode].\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time_model_evaluation(quantized_model_int8, device='cuda')\n",
    "\"\"\"\n",
    "Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. \n",
    "This could be because the operator doesn't exist for this backend, or was omitted during \n",
    "the selective/custom build process (if using custom build). \n",
    "If you are a Facebook employee using PyTorch on mobile, \n",
    "please visit https://fburl.com/ptmfixes for possible resolutions. \n",
    "'quantized::linear_dynamic' is only available for these backends: [CPU, BackendSelect, Named, \n",
    "ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, \n",
    "AutogradMLC, Tracer, Autocast, Batched, VmapMode].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-spencer",
   "metadata": {},
   "source": [
    "| Model | device | Accuracy | Time |\n",
    "| --- | --- | --- |--- |\n",
    "| original fp32 | cpu | .918 | 116 |\n",
    "| original fp32 | gpu | .918 | 17 |\n",
    "| quantized int8 | cpu | .919 | 83 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-beverage",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "the above results , were measured in the below config.  \n",
    "You might need to reduce the OpenMP threads, if introducing parralelism in your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "endless-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 8\n",
      "\tat::get_num_interop_threads() : 8\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 8\n",
      "Intel(R) oneAPI Math Kernel Library Version 2021.3-Product Build 20210617 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 8\n",
      "Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n",
      "std::thread::hardware_concurrency() : 16\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ( torch.__config__.parallel_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-replication",
   "metadata": {},
   "source": [
    "## Optimization: TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-beast",
   "metadata": {},
   "source": [
    "TorchScript is a way to create serializable and optimizable models from PyTorch code\n",
    "\n",
    "load model as torchscript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-learning",
   "metadata": {},
   "source": [
    "According to [Hugging Face docs](https://huggingface.co/transformers/torchscript.html), the `torchscript` flag\n",
    "\n",
    ">This flag is necessary because most of the language models in this repository have tied weights between their Embedding layer and their Decoding layer. \n",
    ">\n",
    ">TorchScript does not allow the export of models that have tied weights, it is therefore necessary to untie the weights beforehand.\n",
    ">\n",
    ">This implies that models instantiated with the torchscript flag have their Embedding layer and Decoding layer separate, which means that they should not be trained down the line. Training would de-synchronize the two layers, leading to unexpected results.\n",
    ">\n",
    ">This is not the case for models that do not have a Language Model head, as those do not have tied weights. \n",
    "These models can be safely exported without the torchscript flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "earned-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir, torchscript=True\n",
    "                                                                               , return_dict=False)\n",
    "script_tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir , torchscript=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-strategy",
   "metadata": {},
   "source": [
    "create a dummy input to pass to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "needed-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"men shoes\"\n",
    "res = script_tokenizer.encode_plus(text, return_tensors=\"pt\", padding=\"max_length\",truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "toxic-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = res['input_ids']\n",
    "masks_tensors = res['attention_mask']\n",
    "\n",
    "dummy_input = [tokens_tensor, masks_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "applicable-anchor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 2273, 6007,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-liquid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hindu-pricing",
   "metadata": {},
   "source": [
    "Creating the trace by passing a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "superb-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = script_model.to(device)\n",
    "\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(script_model, [tokens_tensor, masks_tensors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "celtic-volume",
   "metadata": {},
   "source": [
    "Creating the trace of quantized model by passing a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "floating-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = script_model.to('cpu')\n",
    "quantized_model_int8 = torch.quantization.quantize_dynamic(\n",
    "    script_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "\n",
    "quantized_model_int8 = quantized_model_int8.to('cpu')\n",
    "dummy_input = (tokens_tensor.to('cpu'), masks_tensors.to('cpu') )\n",
    "quantized_model_int8 = quantized_model_int8.to('cpu')\n",
    "traced_model_int8 = torch.jit.trace(quantized_model_int8, dummy_input )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-corpus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "micro-scheme",
   "metadata": {},
   "source": [
    "lets compute how long it takes to predict with traced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "enabling-trance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.97}\n",
      "Evaluate total time (ms): 13128.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(traced_model, device='cpu')\n",
    "#prediction_batch(traced_model,test_dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "inclusive-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 ms ± 1.68 ms per loop (mean ± std. dev. of 3 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 5\n",
    "time_model_evaluation_single(traced_model, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "creative-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.97}\n",
      "Evaluate total time (ms): 3444.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(traced_model, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "absent-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 ms ± 1.43 ms per loop (mean ± std. dev. of 3 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 5\n",
    "time_model_evaluation_single(traced_model, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "particular-personality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 ms ± 2.75 ms per loop (mean ± std. dev. of 3 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 5\n",
    "time_model_evaluation_single(traced_model_int8, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-violation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "southern-breakdown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'SHOES': 0.8049256801605225,\n",
       "  'SANDAL': 0.06582888960838318,\n",
       "  'GROCERY': 0.060891248285770416,\n",
       "  'HEALTH_PERSONAL_CARE': 0.030060164630413055,\n",
       "  'BOOT': 0.02072826586663723}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.prediction( model=traced_model_int8\n",
    "                 , tokens_tensor=tokens_tensor\n",
    "                 , masks_tensors=masks_tensors \n",
    "                 , id2label_str=id2label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-event",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-coffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "japanese-bennett",
   "metadata": {},
   "source": [
    "| Model | device | Accuracy | Time |\n",
    "| --- | --- | --- |--- |\n",
    "| original fp32 | cpu | .918 | 130 |\n",
    "| original fp32 (trace) | cpu | .918 | 124 |\n",
    "| original fp32 | gpu | .918 | 22 |\n",
    "| original fp32 (trace) | gpu | .918 | 18 |\n",
    "| quantized int8 | cpu | .919 | 104 |\n",
    "| quantized int8 (trace) | cpu | .919 | 89.5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-pension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-conservative",
   "metadata": {},
   "source": [
    "Save / load  traced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "spatial-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_trace = f'{model_dir}__trace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "municipal-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {model_dir_trace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "gorgeous-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(traced_model, f\"{model_dir_trace}/traced_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-accuracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efficient-spending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=DistilBertForSequenceClassification\n",
       "  (distilbert): RecursiveScriptModule(\n",
       "    original_name=DistilBertModel\n",
       "    (embeddings): RecursiveScriptModule(\n",
       "      original_name=Embeddings\n",
       "      (word_embeddings): RecursiveScriptModule(original_name=Embedding)\n",
       "      (position_embeddings): RecursiveScriptModule(original_name=Embedding)\n",
       "      (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "    (transformer): RecursiveScriptModule(\n",
       "      original_name=Transformer\n",
       "      (layer): RecursiveScriptModule(\n",
       "        original_name=ModuleList\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=TransformerBlock\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=MultiHeadSelfAttention\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (q_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (k_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (v_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (out_lin): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (sa_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (ffn): RecursiveScriptModule(\n",
       "            original_name=FFN\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (lin1): RecursiveScriptModule(original_name=Linear)\n",
       "            (lin2): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (output_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=TransformerBlock\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=MultiHeadSelfAttention\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (q_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (k_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (v_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (out_lin): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (sa_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (ffn): RecursiveScriptModule(\n",
       "            original_name=FFN\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (lin1): RecursiveScriptModule(original_name=Linear)\n",
       "            (lin2): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (output_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=TransformerBlock\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=MultiHeadSelfAttention\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (q_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (k_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (v_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (out_lin): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (sa_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (ffn): RecursiveScriptModule(\n",
       "            original_name=FFN\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (lin1): RecursiveScriptModule(original_name=Linear)\n",
       "            (lin2): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (output_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=TransformerBlock\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=MultiHeadSelfAttention\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (q_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (k_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (v_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (out_lin): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (sa_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (ffn): RecursiveScriptModule(\n",
       "            original_name=FFN\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (lin1): RecursiveScriptModule(original_name=Linear)\n",
       "            (lin2): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (output_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=TransformerBlock\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=MultiHeadSelfAttention\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (q_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (k_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (v_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (out_lin): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (sa_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (ffn): RecursiveScriptModule(\n",
       "            original_name=FFN\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (lin1): RecursiveScriptModule(original_name=Linear)\n",
       "            (lin2): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (output_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=TransformerBlock\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=MultiHeadSelfAttention\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (q_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (k_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (v_lin): RecursiveScriptModule(original_name=Linear)\n",
       "            (out_lin): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (sa_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (ffn): RecursiveScriptModule(\n",
       "            original_name=FFN\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            (lin1): RecursiveScriptModule(original_name=Linear)\n",
       "            (lin2): RecursiveScriptModule(original_name=Linear)\n",
       "          )\n",
       "          (output_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): RecursiveScriptModule(original_name=Linear)\n",
       "  (classifier): RecursiveScriptModule(original_name=Linear)\n",
       "  (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.jit.load( f\"{model_dir_trace}/traced_model.pt\")\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-summary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-armor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-gibson",
   "metadata": {},
   "source": [
    "## Save Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-monitor",
   "metadata": {},
   "source": [
    "For the next section, let us save the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "integral-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir}/index_to_name.json','w') as f:\n",
    "    json.dump(id2label_str,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "elect-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir_trace}/index_to_name.json','w') as f:\n",
    "    json.dump(id2label_str,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-composition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "civic-zimbabwe",
   "metadata": {},
   "source": [
    "in a separate json config, let use store things like \n",
    "- was lower case used by the tokenizer\n",
    "- max length of tokenizer\n",
    "- num labels\n",
    "- is the model a jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "statutory-conference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "appreciated-accent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "together-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../artifacts/model/distilbert-base-uncased__trace/tokenizer_config.json',\n",
       " '../artifacts/model/distilbert-base-uncased__trace/special_tokens_map.json',\n",
       " '../artifacts/model/distilbert-base-uncased__trace/vocab.txt',\n",
       " '../artifacts/model/distilbert-base-uncased__trace/added_tokens.json',\n",
       " '../artifacts/model/distilbert-base-uncased__trace/tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_dir_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "involved-spencer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'pt-original',\n",
       " 'do_lower_case': True,\n",
       " 'num_labels': 31,\n",
       " 'save_mode': 'original',\n",
       " 'max_length': 512,\n",
       " 'captum_explanation': True,\n",
       " 'base_model': 'distilbert-base-uncased',\n",
       " 'top_k': 5}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_config = {\n",
    " \"model_name\":\"pt-original\",\n",
    " \"do_lower_case\": tokenizer.do_lower_case,\n",
    " \"num_labels\":len(id2label_str),\n",
    " \"save_mode\":\"original\",\n",
    " \"max_length\":tokenizer.model_max_length,\n",
    " \"captum_explanation\": True,\n",
    " \"base_model\": base_model,\n",
    " \"top_k\": 5   \n",
    "\n",
    "}\n",
    "\n",
    "setup_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "violent-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir}/setup_config.json','w') as f:\n",
    "    json.dump(setup_config,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "smart-integration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'pt-jit',\n",
       " 'do_lower_case': True,\n",
       " 'num_labels': 31,\n",
       " 'save_mode': 'jit',\n",
       " 'max_length': 512,\n",
       " 'captum_explanation': False,\n",
       " 'base_model': 'distilbert-base-uncased',\n",
       " 'top_k': 5}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_config_trace = {**setup_config}\n",
    "setup_config_trace['model_name'] = \"pt-jit\"\n",
    "setup_config_trace['captum_explanation'] = False\n",
    "setup_config_trace['save_mode'] = 'jit'\n",
    "setup_config_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "limiting-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir_trace}/setup_config.json','w') as f:\n",
    "    json.dump(setup_config_trace,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-catalyst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-sacramento",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
