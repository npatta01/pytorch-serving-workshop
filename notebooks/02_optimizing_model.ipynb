{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-teach",
   "metadata": {},
   "source": [
    "# Optimizing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-exhibition",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "- Sample code to load artifacts / run inference\n",
    "- Quantizing model\n",
    "- Converting the Bert model with torch script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-hayes",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "forbidden-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "import os\n",
    "import json\n",
    "from ts.utils.util  import map_class_to_label\n",
    "from tqdm import tqdm, trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-pressure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "statistical-absolute",
   "metadata": {},
   "source": [
    "## Load model/tokenizer artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "enclosed-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir ='../artifacts/model/amazon/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-dover",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radio-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experimental-lambda",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"../artifacts/model/amazon/\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"ABIS_DRUGSTORE\",\n",
       "    \"1\": \"ABIS_LAWN_AND_GARDEN\",\n",
       "    \"2\": \"ACCESSORY\",\n",
       "    \"3\": \"ACCESSORY_OR_PART_OR_SUPPLY\",\n",
       "    \"4\": \"AUTO_ACCESSORY\",\n",
       "    \"5\": \"BABY_PRODUCT\",\n",
       "    \"6\": \"BACKPACK\",\n",
       "    \"7\": \"BATTERY\",\n",
       "    \"8\": \"BEAUTY\",\n",
       "    \"9\": \"BED\",\n",
       "    \"10\": \"BED_FRAME\",\n",
       "    \"11\": \"BENCH\",\n",
       "    \"12\": \"BISS\",\n",
       "    \"13\": \"BOOT\",\n",
       "    \"14\": \"BRACELET\",\n",
       "    \"15\": \"BREAD\",\n",
       "    \"16\": \"CABINET\",\n",
       "    \"17\": \"CELLULAR_PHONE_CASE\",\n",
       "    \"18\": \"CHAIR\",\n",
       "    \"19\": \"CHARGING_ADAPTER\",\n",
       "    \"20\": \"CLEANING_AGENT\",\n",
       "    \"21\": \"CLOCK\",\n",
       "    \"22\": \"CLOTHES_HANGER\",\n",
       "    \"23\": \"COFFEE\",\n",
       "    \"24\": \"COMPUTER_ADD_ON\",\n",
       "    \"25\": \"COMPUTER_COMPONENT\",\n",
       "    \"26\": \"DAIRY_BASED_DRINK\",\n",
       "    \"27\": \"DESK\",\n",
       "    \"28\": \"DRINKING_CUP\",\n",
       "    \"29\": \"EARRING\",\n",
       "    \"30\": \"EDIBLE_OIL_VEGETABLE\",\n",
       "    \"31\": \"ELECTRONIC_ADAPTER\",\n",
       "    \"32\": \"FILE_FOLDER\",\n",
       "    \"33\": \"FINEEARRING\",\n",
       "    \"34\": \"FINENECKLACEBRACELETANKLET\",\n",
       "    \"35\": \"FINEOTHER\",\n",
       "    \"36\": \"FINERING\",\n",
       "    \"37\": \"FLAT_SCREEN_DISPLAY_MOUNT\",\n",
       "    \"38\": \"FLAT_SHEET\",\n",
       "    \"39\": \"FOOD_SERVICE_SUPPLY\",\n",
       "    \"40\": \"FURNITURE\",\n",
       "    \"41\": \"FURNITURE_COVER\",\n",
       "    \"42\": \"GROCERY\",\n",
       "    \"43\": \"HANDBAG\",\n",
       "    \"44\": \"HARDWARE\",\n",
       "    \"45\": \"HARDWARE_HANDLE\",\n",
       "    \"46\": \"HAT\",\n",
       "    \"47\": \"HEADBOARD\",\n",
       "    \"48\": \"HEADPHONES\",\n",
       "    \"49\": \"HEALTH_PERSONAL_CARE\",\n",
       "    \"50\": \"HERB\",\n",
       "    \"51\": \"HOME\",\n",
       "    \"52\": \"HOME_BED_AND_BATH\",\n",
       "    \"53\": \"HOME_FURNITURE_AND_DECOR\",\n",
       "    \"54\": \"HOME_LIGHTING_AND_LAMPS\",\n",
       "    \"55\": \"HOME_MIRROR\",\n",
       "    \"56\": \"INSTRUMENT_PARTS_AND_ACCESSORIES\",\n",
       "    \"57\": \"JANITORIAL_SUPPLY\",\n",
       "    \"58\": \"JAR\",\n",
       "    \"59\": \"KITCHEN\",\n",
       "    \"60\": \"LABEL\",\n",
       "    \"61\": \"LAMP\",\n",
       "    \"62\": \"LEGUME\",\n",
       "    \"63\": \"LIGHT_BULB\",\n",
       "    \"64\": \"LIGHT_FIXTURE\",\n",
       "    \"65\": \"LUGGAGE\",\n",
       "    \"66\": \"MEDICATION\",\n",
       "    \"67\": \"NECKLACE\",\n",
       "    \"68\": \"NUTRITIONAL_SUPPLEMENT\",\n",
       "    \"69\": \"OFFICE_ELECTRONICS\",\n",
       "    \"70\": \"OFFICE_PRODUCTS\",\n",
       "    \"71\": \"OTTOMAN\",\n",
       "    \"72\": \"OUTDOOR_LIVING\",\n",
       "    \"73\": \"PANTRY\",\n",
       "    \"74\": \"PET_SUPPLIES\",\n",
       "    \"75\": \"PILLOW\",\n",
       "    \"76\": \"PLANTER\",\n",
       "    \"77\": \"PLUMBING_FIXTURE\",\n",
       "    \"78\": \"PORTABLE_ELECTRONIC_DEVICE_COVER\",\n",
       "    \"79\": \"RECREATION_BALL\",\n",
       "    \"80\": \"RING\",\n",
       "    \"81\": \"RUG\",\n",
       "    \"82\": \"SAFETY_SUPPLY\",\n",
       "    \"83\": \"SANDAL\",\n",
       "    \"84\": \"SAUCE\",\n",
       "    \"85\": \"SAUTE_FRY_PAN\",\n",
       "    \"86\": \"SCREEN_PROTECTOR\",\n",
       "    \"87\": \"SHELF\",\n",
       "    \"88\": \"SHOES\",\n",
       "    \"89\": \"SKIN_CLEANING_AGENT\",\n",
       "    \"90\": \"SKIN_MOISTURIZER\",\n",
       "    \"91\": \"SOFA\",\n",
       "    \"92\": \"SPEAKERS\",\n",
       "    \"93\": \"SPORTING_GOODS\",\n",
       "    \"94\": \"STOOL_SEATING\",\n",
       "    \"95\": \"STORAGE_BAG\",\n",
       "    \"96\": \"STORAGE_BINDER\",\n",
       "    \"97\": \"STORAGE_BOX\",\n",
       "    \"98\": \"STORAGE_HOOK\",\n",
       "    \"99\": \"SUITCASE\",\n",
       "    \"100\": \"TABLE\",\n",
       "    \"101\": \"TEA\",\n",
       "    \"102\": \"THERMOPLASTIC_FILAMENT\",\n",
       "    \"103\": \"TOOLS\",\n",
       "    \"104\": \"TOWEL_HOLDER\",\n",
       "    \"105\": \"UMBRELLA\",\n",
       "    \"106\": \"VITAMIN\",\n",
       "    \"107\": \"WALLET\",\n",
       "    \"108\": \"WALL_ART\",\n",
       "    \"109\": \"WASTE_BAG\",\n",
       "    \"110\": \"WIRELESS_ACCESSORY\",\n",
       "    \"111\": \"WRITING_INSTRUMENT\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"ABIS_DRUGSTORE\": 0,\n",
       "    \"ABIS_LAWN_AND_GARDEN\": 1,\n",
       "    \"ACCESSORY\": 2,\n",
       "    \"ACCESSORY_OR_PART_OR_SUPPLY\": 3,\n",
       "    \"AUTO_ACCESSORY\": 4,\n",
       "    \"BABY_PRODUCT\": 5,\n",
       "    \"BACKPACK\": 6,\n",
       "    \"BATTERY\": 7,\n",
       "    \"BEAUTY\": 8,\n",
       "    \"BED\": 9,\n",
       "    \"BED_FRAME\": 10,\n",
       "    \"BENCH\": 11,\n",
       "    \"BISS\": 12,\n",
       "    \"BOOT\": 13,\n",
       "    \"BRACELET\": 14,\n",
       "    \"BREAD\": 15,\n",
       "    \"CABINET\": 16,\n",
       "    \"CELLULAR_PHONE_CASE\": 17,\n",
       "    \"CHAIR\": 18,\n",
       "    \"CHARGING_ADAPTER\": 19,\n",
       "    \"CLEANING_AGENT\": 20,\n",
       "    \"CLOCK\": 21,\n",
       "    \"CLOTHES_HANGER\": 22,\n",
       "    \"COFFEE\": 23,\n",
       "    \"COMPUTER_ADD_ON\": 24,\n",
       "    \"COMPUTER_COMPONENT\": 25,\n",
       "    \"DAIRY_BASED_DRINK\": 26,\n",
       "    \"DESK\": 27,\n",
       "    \"DRINKING_CUP\": 28,\n",
       "    \"EARRING\": 29,\n",
       "    \"EDIBLE_OIL_VEGETABLE\": 30,\n",
       "    \"ELECTRONIC_ADAPTER\": 31,\n",
       "    \"FILE_FOLDER\": 32,\n",
       "    \"FINEEARRING\": 33,\n",
       "    \"FINENECKLACEBRACELETANKLET\": 34,\n",
       "    \"FINEOTHER\": 35,\n",
       "    \"FINERING\": 36,\n",
       "    \"FLAT_SCREEN_DISPLAY_MOUNT\": 37,\n",
       "    \"FLAT_SHEET\": 38,\n",
       "    \"FOOD_SERVICE_SUPPLY\": 39,\n",
       "    \"FURNITURE\": 40,\n",
       "    \"FURNITURE_COVER\": 41,\n",
       "    \"GROCERY\": 42,\n",
       "    \"HANDBAG\": 43,\n",
       "    \"HARDWARE\": 44,\n",
       "    \"HARDWARE_HANDLE\": 45,\n",
       "    \"HAT\": 46,\n",
       "    \"HEADBOARD\": 47,\n",
       "    \"HEADPHONES\": 48,\n",
       "    \"HEALTH_PERSONAL_CARE\": 49,\n",
       "    \"HERB\": 50,\n",
       "    \"HOME\": 51,\n",
       "    \"HOME_BED_AND_BATH\": 52,\n",
       "    \"HOME_FURNITURE_AND_DECOR\": 53,\n",
       "    \"HOME_LIGHTING_AND_LAMPS\": 54,\n",
       "    \"HOME_MIRROR\": 55,\n",
       "    \"INSTRUMENT_PARTS_AND_ACCESSORIES\": 56,\n",
       "    \"JANITORIAL_SUPPLY\": 57,\n",
       "    \"JAR\": 58,\n",
       "    \"KITCHEN\": 59,\n",
       "    \"LABEL\": 60,\n",
       "    \"LAMP\": 61,\n",
       "    \"LEGUME\": 62,\n",
       "    \"LIGHT_BULB\": 63,\n",
       "    \"LIGHT_FIXTURE\": 64,\n",
       "    \"LUGGAGE\": 65,\n",
       "    \"MEDICATION\": 66,\n",
       "    \"NECKLACE\": 67,\n",
       "    \"NUTRITIONAL_SUPPLEMENT\": 68,\n",
       "    \"OFFICE_ELECTRONICS\": 69,\n",
       "    \"OFFICE_PRODUCTS\": 70,\n",
       "    \"OTTOMAN\": 71,\n",
       "    \"OUTDOOR_LIVING\": 72,\n",
       "    \"PANTRY\": 73,\n",
       "    \"PET_SUPPLIES\": 74,\n",
       "    \"PILLOW\": 75,\n",
       "    \"PLANTER\": 76,\n",
       "    \"PLUMBING_FIXTURE\": 77,\n",
       "    \"PORTABLE_ELECTRONIC_DEVICE_COVER\": 78,\n",
       "    \"RECREATION_BALL\": 79,\n",
       "    \"RING\": 80,\n",
       "    \"RUG\": 81,\n",
       "    \"SAFETY_SUPPLY\": 82,\n",
       "    \"SANDAL\": 83,\n",
       "    \"SAUCE\": 84,\n",
       "    \"SAUTE_FRY_PAN\": 85,\n",
       "    \"SCREEN_PROTECTOR\": 86,\n",
       "    \"SHELF\": 87,\n",
       "    \"SHOES\": 88,\n",
       "    \"SKIN_CLEANING_AGENT\": 89,\n",
       "    \"SKIN_MOISTURIZER\": 90,\n",
       "    \"SOFA\": 91,\n",
       "    \"SPEAKERS\": 92,\n",
       "    \"SPORTING_GOODS\": 93,\n",
       "    \"STOOL_SEATING\": 94,\n",
       "    \"STORAGE_BAG\": 95,\n",
       "    \"STORAGE_BINDER\": 96,\n",
       "    \"STORAGE_BOX\": 97,\n",
       "    \"STORAGE_HOOK\": 98,\n",
       "    \"SUITCASE\": 99,\n",
       "    \"TABLE\": 100,\n",
       "    \"TEA\": 101,\n",
       "    \"THERMOPLASTIC_FILAMENT\": 102,\n",
       "    \"TOOLS\": 103,\n",
       "    \"TOWEL_HOLDER\": 104,\n",
       "    \"UMBRELLA\": 105,\n",
       "    \"VITAMIN\": 106,\n",
       "    \"WALLET\": 107,\n",
       "    \"WALL_ART\": 108,\n",
       "    \"WASTE_BAG\": 109,\n",
       "    \"WIRELESS_ACCESSORY\": 110,\n",
       "    \"WRITING_INSTRUMENT\": 111\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.11.1\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-reference",
   "metadata": {},
   "source": [
    "model label name to id and id->label name mapping is stord as part of model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacterial-athens",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABIS_DRUGSTORE': 0,\n",
       " 'ABIS_LAWN_AND_GARDEN': 1,\n",
       " 'ACCESSORY': 2,\n",
       " 'ACCESSORY_OR_PART_OR_SUPPLY': 3,\n",
       " 'AUTO_ACCESSORY': 4,\n",
       " 'BABY_PRODUCT': 5,\n",
       " 'BACKPACK': 6,\n",
       " 'BATTERY': 7,\n",
       " 'BEAUTY': 8,\n",
       " 'BED': 9,\n",
       " 'BED_FRAME': 10,\n",
       " 'BENCH': 11,\n",
       " 'BISS': 12,\n",
       " 'BOOT': 13,\n",
       " 'BRACELET': 14,\n",
       " 'BREAD': 15,\n",
       " 'CABINET': 16,\n",
       " 'CELLULAR_PHONE_CASE': 17,\n",
       " 'CHAIR': 18,\n",
       " 'CHARGING_ADAPTER': 19,\n",
       " 'CLEANING_AGENT': 20,\n",
       " 'CLOCK': 21,\n",
       " 'CLOTHES_HANGER': 22,\n",
       " 'COFFEE': 23,\n",
       " 'COMPUTER_ADD_ON': 24,\n",
       " 'COMPUTER_COMPONENT': 25,\n",
       " 'DAIRY_BASED_DRINK': 26,\n",
       " 'DESK': 27,\n",
       " 'DRINKING_CUP': 28,\n",
       " 'EARRING': 29,\n",
       " 'EDIBLE_OIL_VEGETABLE': 30,\n",
       " 'ELECTRONIC_ADAPTER': 31,\n",
       " 'FILE_FOLDER': 32,\n",
       " 'FINEEARRING': 33,\n",
       " 'FINENECKLACEBRACELETANKLET': 34,\n",
       " 'FINEOTHER': 35,\n",
       " 'FINERING': 36,\n",
       " 'FLAT_SCREEN_DISPLAY_MOUNT': 37,\n",
       " 'FLAT_SHEET': 38,\n",
       " 'FOOD_SERVICE_SUPPLY': 39,\n",
       " 'FURNITURE': 40,\n",
       " 'FURNITURE_COVER': 41,\n",
       " 'GROCERY': 42,\n",
       " 'HANDBAG': 43,\n",
       " 'HARDWARE': 44,\n",
       " 'HARDWARE_HANDLE': 45,\n",
       " 'HAT': 46,\n",
       " 'HEADBOARD': 47,\n",
       " 'HEADPHONES': 48,\n",
       " 'HEALTH_PERSONAL_CARE': 49,\n",
       " 'HERB': 50,\n",
       " 'HOME': 51,\n",
       " 'HOME_BED_AND_BATH': 52,\n",
       " 'HOME_FURNITURE_AND_DECOR': 53,\n",
       " 'HOME_LIGHTING_AND_LAMPS': 54,\n",
       " 'HOME_MIRROR': 55,\n",
       " 'INSTRUMENT_PARTS_AND_ACCESSORIES': 56,\n",
       " 'JANITORIAL_SUPPLY': 57,\n",
       " 'JAR': 58,\n",
       " 'KITCHEN': 59,\n",
       " 'LABEL': 60,\n",
       " 'LAMP': 61,\n",
       " 'LEGUME': 62,\n",
       " 'LIGHT_BULB': 63,\n",
       " 'LIGHT_FIXTURE': 64,\n",
       " 'LUGGAGE': 65,\n",
       " 'MEDICATION': 66,\n",
       " 'NECKLACE': 67,\n",
       " 'NUTRITIONAL_SUPPLEMENT': 68,\n",
       " 'OFFICE_ELECTRONICS': 69,\n",
       " 'OFFICE_PRODUCTS': 70,\n",
       " 'OTTOMAN': 71,\n",
       " 'OUTDOOR_LIVING': 72,\n",
       " 'PANTRY': 73,\n",
       " 'PET_SUPPLIES': 74,\n",
       " 'PILLOW': 75,\n",
       " 'PLANTER': 76,\n",
       " 'PLUMBING_FIXTURE': 77,\n",
       " 'PORTABLE_ELECTRONIC_DEVICE_COVER': 78,\n",
       " 'RECREATION_BALL': 79,\n",
       " 'RING': 80,\n",
       " 'RUG': 81,\n",
       " 'SAFETY_SUPPLY': 82,\n",
       " 'SANDAL': 83,\n",
       " 'SAUCE': 84,\n",
       " 'SAUTE_FRY_PAN': 85,\n",
       " 'SCREEN_PROTECTOR': 86,\n",
       " 'SHELF': 87,\n",
       " 'SHOES': 88,\n",
       " 'SKIN_CLEANING_AGENT': 89,\n",
       " 'SKIN_MOISTURIZER': 90,\n",
       " 'SOFA': 91,\n",
       " 'SPEAKERS': 92,\n",
       " 'SPORTING_GOODS': 93,\n",
       " 'STOOL_SEATING': 94,\n",
       " 'STORAGE_BAG': 95,\n",
       " 'STORAGE_BINDER': 96,\n",
       " 'STORAGE_BOX': 97,\n",
       " 'STORAGE_HOOK': 98,\n",
       " 'SUITCASE': 99,\n",
       " 'TABLE': 100,\n",
       " 'TEA': 101,\n",
       " 'THERMOPLASTIC_FILAMENT': 102,\n",
       " 'TOOLS': 103,\n",
       " 'TOWEL_HOLDER': 104,\n",
       " 'UMBRELLA': 105,\n",
       " 'VITAMIN': 106,\n",
       " 'WALLET': 107,\n",
       " 'WALL_ART': 108,\n",
       " 'WASTE_BAG': 109,\n",
       " 'WIRELESS_ACCESSORY': 110,\n",
       " 'WRITING_INSTRUMENT': 111}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model.config.label2id\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "micro-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "curious-shakespeare",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ABIS_DRUGSTORE',\n",
       " 1: 'ABIS_LAWN_AND_GARDEN',\n",
       " 2: 'ACCESSORY',\n",
       " 3: 'ACCESSORY_OR_PART_OR_SUPPLY',\n",
       " 4: 'AUTO_ACCESSORY',\n",
       " 5: 'BABY_PRODUCT',\n",
       " 6: 'BACKPACK',\n",
       " 7: 'BATTERY',\n",
       " 8: 'BEAUTY',\n",
       " 9: 'BED',\n",
       " 10: 'BED_FRAME',\n",
       " 11: 'BENCH',\n",
       " 12: 'BISS',\n",
       " 13: 'BOOT',\n",
       " 14: 'BRACELET',\n",
       " 15: 'BREAD',\n",
       " 16: 'CABINET',\n",
       " 17: 'CELLULAR_PHONE_CASE',\n",
       " 18: 'CHAIR',\n",
       " 19: 'CHARGING_ADAPTER',\n",
       " 20: 'CLEANING_AGENT',\n",
       " 21: 'CLOCK',\n",
       " 22: 'CLOTHES_HANGER',\n",
       " 23: 'COFFEE',\n",
       " 24: 'COMPUTER_ADD_ON',\n",
       " 25: 'COMPUTER_COMPONENT',\n",
       " 26: 'DAIRY_BASED_DRINK',\n",
       " 27: 'DESK',\n",
       " 28: 'DRINKING_CUP',\n",
       " 29: 'EARRING',\n",
       " 30: 'EDIBLE_OIL_VEGETABLE',\n",
       " 31: 'ELECTRONIC_ADAPTER',\n",
       " 32: 'FILE_FOLDER',\n",
       " 33: 'FINEEARRING',\n",
       " 34: 'FINENECKLACEBRACELETANKLET',\n",
       " 35: 'FINEOTHER',\n",
       " 36: 'FINERING',\n",
       " 37: 'FLAT_SCREEN_DISPLAY_MOUNT',\n",
       " 38: 'FLAT_SHEET',\n",
       " 39: 'FOOD_SERVICE_SUPPLY',\n",
       " 40: 'FURNITURE',\n",
       " 41: 'FURNITURE_COVER',\n",
       " 42: 'GROCERY',\n",
       " 43: 'HANDBAG',\n",
       " 44: 'HARDWARE',\n",
       " 45: 'HARDWARE_HANDLE',\n",
       " 46: 'HAT',\n",
       " 47: 'HEADBOARD',\n",
       " 48: 'HEADPHONES',\n",
       " 49: 'HEALTH_PERSONAL_CARE',\n",
       " 50: 'HERB',\n",
       " 51: 'HOME',\n",
       " 52: 'HOME_BED_AND_BATH',\n",
       " 53: 'HOME_FURNITURE_AND_DECOR',\n",
       " 54: 'HOME_LIGHTING_AND_LAMPS',\n",
       " 55: 'HOME_MIRROR',\n",
       " 56: 'INSTRUMENT_PARTS_AND_ACCESSORIES',\n",
       " 57: 'JANITORIAL_SUPPLY',\n",
       " 58: 'JAR',\n",
       " 59: 'KITCHEN',\n",
       " 60: 'LABEL',\n",
       " 61: 'LAMP',\n",
       " 62: 'LEGUME',\n",
       " 63: 'LIGHT_BULB',\n",
       " 64: 'LIGHT_FIXTURE',\n",
       " 65: 'LUGGAGE',\n",
       " 66: 'MEDICATION',\n",
       " 67: 'NECKLACE',\n",
       " 68: 'NUTRITIONAL_SUPPLEMENT',\n",
       " 69: 'OFFICE_ELECTRONICS',\n",
       " 70: 'OFFICE_PRODUCTS',\n",
       " 71: 'OTTOMAN',\n",
       " 72: 'OUTDOOR_LIVING',\n",
       " 73: 'PANTRY',\n",
       " 74: 'PET_SUPPLIES',\n",
       " 75: 'PILLOW',\n",
       " 76: 'PLANTER',\n",
       " 77: 'PLUMBING_FIXTURE',\n",
       " 78: 'PORTABLE_ELECTRONIC_DEVICE_COVER',\n",
       " 79: 'RECREATION_BALL',\n",
       " 80: 'RING',\n",
       " 81: 'RUG',\n",
       " 82: 'SAFETY_SUPPLY',\n",
       " 83: 'SANDAL',\n",
       " 84: 'SAUCE',\n",
       " 85: 'SAUTE_FRY_PAN',\n",
       " 86: 'SCREEN_PROTECTOR',\n",
       " 87: 'SHELF',\n",
       " 88: 'SHOES',\n",
       " 89: 'SKIN_CLEANING_AGENT',\n",
       " 90: 'SKIN_MOISTURIZER',\n",
       " 91: 'SOFA',\n",
       " 92: 'SPEAKERS',\n",
       " 93: 'SPORTING_GOODS',\n",
       " 94: 'STOOL_SEATING',\n",
       " 95: 'STORAGE_BAG',\n",
       " 96: 'STORAGE_BINDER',\n",
       " 97: 'STORAGE_BOX',\n",
       " 98: 'STORAGE_HOOK',\n",
       " 99: 'SUITCASE',\n",
       " 100: 'TABLE',\n",
       " 101: 'TEA',\n",
       " 102: 'THERMOPLASTIC_FILAMENT',\n",
       " 103: 'TOOLS',\n",
       " 104: 'TOWEL_HOLDER',\n",
       " 105: 'UMBRELLA',\n",
       " 106: 'VITAMIN',\n",
       " 107: 'WALLET',\n",
       " 108: 'WALL_ART',\n",
       " 109: 'WASTE_BAG',\n",
       " 110: 'WIRELESS_ACCESSORY',\n",
       " 111: 'WRITING_INSTRUMENT'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-signal",
   "metadata": {},
   "source": [
    "a utiliy method provided by torchserver requires labels to also be strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loose-result",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'ABIS_DRUGSTORE',\n",
       " '1': 'ABIS_LAWN_AND_GARDEN',\n",
       " '2': 'ACCESSORY',\n",
       " '3': 'ACCESSORY_OR_PART_OR_SUPPLY',\n",
       " '4': 'AUTO_ACCESSORY',\n",
       " '5': 'BABY_PRODUCT',\n",
       " '6': 'BACKPACK',\n",
       " '7': 'BATTERY',\n",
       " '8': 'BEAUTY',\n",
       " '9': 'BED',\n",
       " '10': 'BED_FRAME',\n",
       " '11': 'BENCH',\n",
       " '12': 'BISS',\n",
       " '13': 'BOOT',\n",
       " '14': 'BRACELET',\n",
       " '15': 'BREAD',\n",
       " '16': 'CABINET',\n",
       " '17': 'CELLULAR_PHONE_CASE',\n",
       " '18': 'CHAIR',\n",
       " '19': 'CHARGING_ADAPTER',\n",
       " '20': 'CLEANING_AGENT',\n",
       " '21': 'CLOCK',\n",
       " '22': 'CLOTHES_HANGER',\n",
       " '23': 'COFFEE',\n",
       " '24': 'COMPUTER_ADD_ON',\n",
       " '25': 'COMPUTER_COMPONENT',\n",
       " '26': 'DAIRY_BASED_DRINK',\n",
       " '27': 'DESK',\n",
       " '28': 'DRINKING_CUP',\n",
       " '29': 'EARRING',\n",
       " '30': 'EDIBLE_OIL_VEGETABLE',\n",
       " '31': 'ELECTRONIC_ADAPTER',\n",
       " '32': 'FILE_FOLDER',\n",
       " '33': 'FINEEARRING',\n",
       " '34': 'FINENECKLACEBRACELETANKLET',\n",
       " '35': 'FINEOTHER',\n",
       " '36': 'FINERING',\n",
       " '37': 'FLAT_SCREEN_DISPLAY_MOUNT',\n",
       " '38': 'FLAT_SHEET',\n",
       " '39': 'FOOD_SERVICE_SUPPLY',\n",
       " '40': 'FURNITURE',\n",
       " '41': 'FURNITURE_COVER',\n",
       " '42': 'GROCERY',\n",
       " '43': 'HANDBAG',\n",
       " '44': 'HARDWARE',\n",
       " '45': 'HARDWARE_HANDLE',\n",
       " '46': 'HAT',\n",
       " '47': 'HEADBOARD',\n",
       " '48': 'HEADPHONES',\n",
       " '49': 'HEALTH_PERSONAL_CARE',\n",
       " '50': 'HERB',\n",
       " '51': 'HOME',\n",
       " '52': 'HOME_BED_AND_BATH',\n",
       " '53': 'HOME_FURNITURE_AND_DECOR',\n",
       " '54': 'HOME_LIGHTING_AND_LAMPS',\n",
       " '55': 'HOME_MIRROR',\n",
       " '56': 'INSTRUMENT_PARTS_AND_ACCESSORIES',\n",
       " '57': 'JANITORIAL_SUPPLY',\n",
       " '58': 'JAR',\n",
       " '59': 'KITCHEN',\n",
       " '60': 'LABEL',\n",
       " '61': 'LAMP',\n",
       " '62': 'LEGUME',\n",
       " '63': 'LIGHT_BULB',\n",
       " '64': 'LIGHT_FIXTURE',\n",
       " '65': 'LUGGAGE',\n",
       " '66': 'MEDICATION',\n",
       " '67': 'NECKLACE',\n",
       " '68': 'NUTRITIONAL_SUPPLEMENT',\n",
       " '69': 'OFFICE_ELECTRONICS',\n",
       " '70': 'OFFICE_PRODUCTS',\n",
       " '71': 'OTTOMAN',\n",
       " '72': 'OUTDOOR_LIVING',\n",
       " '73': 'PANTRY',\n",
       " '74': 'PET_SUPPLIES',\n",
       " '75': 'PILLOW',\n",
       " '76': 'PLANTER',\n",
       " '77': 'PLUMBING_FIXTURE',\n",
       " '78': 'PORTABLE_ELECTRONIC_DEVICE_COVER',\n",
       " '79': 'RECREATION_BALL',\n",
       " '80': 'RING',\n",
       " '81': 'RUG',\n",
       " '82': 'SAFETY_SUPPLY',\n",
       " '83': 'SANDAL',\n",
       " '84': 'SAUCE',\n",
       " '85': 'SAUTE_FRY_PAN',\n",
       " '86': 'SCREEN_PROTECTOR',\n",
       " '87': 'SHELF',\n",
       " '88': 'SHOES',\n",
       " '89': 'SKIN_CLEANING_AGENT',\n",
       " '90': 'SKIN_MOISTURIZER',\n",
       " '91': 'SOFA',\n",
       " '92': 'SPEAKERS',\n",
       " '93': 'SPORTING_GOODS',\n",
       " '94': 'STOOL_SEATING',\n",
       " '95': 'STORAGE_BAG',\n",
       " '96': 'STORAGE_BINDER',\n",
       " '97': 'STORAGE_BOX',\n",
       " '98': 'STORAGE_HOOK',\n",
       " '99': 'SUITCASE',\n",
       " '100': 'TABLE',\n",
       " '101': 'TEA',\n",
       " '102': 'THERMOPLASTIC_FILAMENT',\n",
       " '103': 'TOOLS',\n",
       " '104': 'TOWEL_HOLDER',\n",
       " '105': 'UMBRELLA',\n",
       " '106': 'VITAMIN',\n",
       " '107': 'WALLET',\n",
       " '108': 'WALL_ART',\n",
       " '109': 'WASTE_BAG',\n",
       " '110': 'WIRELESS_ACCESSORY',\n",
       " '111': 'WRITING_INSTRUMENT'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_str = {str(key): value for key, value in id2label.items()}\n",
    "id2label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-december",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-massachusetts",
   "metadata": {},
   "source": [
    "load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "controlled-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "                model_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-marine",
   "metadata": {},
   "source": [
    "use gpu if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dental-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-canyon",
   "metadata": {},
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-thought",
   "metadata": {},
   "source": [
    "query we are predicting on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "regional-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'men shoes for work'\n",
    "query  = \"men vitamins\"\n",
    "query  = \"herbal TEA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-shift",
   "metadata": {},
   "source": [
    "get input and attention mask from the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "medium-scholarship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 27849,  5572,   102]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tokenizer.encode_plus(query, return_tensors=\"pt\",  padding=\"max_length\", truncation=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abandoned-height",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86.9 ms, sys: 25.9 ms, total: 113 ms\n",
      "Wall time: 15.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ -7.6677,  -8.9506,  -7.0845,  -5.2786,  -7.1656,  -6.1171,  -9.9482,\n",
       "          -9.1212,  -2.7499,  -7.2937,  -8.6023,  -8.4928,  -5.4319,  -8.9995,\n",
       "          -5.5602,  -1.6690,  -6.5178,  -8.3231,  -7.7803,  -9.7673,  -6.5894,\n",
       "          -8.0236, -10.5098,   0.4534,  -8.5425,  -8.0849,  -3.6514,  -8.6150,\n",
       "          -3.0538,  -7.3485,  -1.2094,  -8.9694, -10.4731,  -6.9120,  -5.5614,\n",
       "          -6.1381,  -8.4838,  -9.5964,  -9.0998,  -4.2063,  -5.1919,  -8.1918,\n",
       "           4.2027,  -7.9540,  -6.4338,  -8.7222,  -7.1057,  -9.4445,  -9.8354,\n",
       "          -2.9263,   0.4510,  -5.4933,  -5.2048,  -6.2273,  -5.9204,  -8.1743,\n",
       "         -10.3333,  -7.3300,  -8.0885,  -5.3360, -11.0123,  -7.2948,  -2.0672,\n",
       "          -8.7549,  -8.7169,  -8.9316,  -4.8772,  -6.4113,  -4.3336, -11.0367,\n",
       "          -7.4192,  -9.4832,  -6.4128,  -3.0107,  -5.8093,  -6.4888,  -7.1651,\n",
       "          -7.2252,  -9.9273,  -9.3188,  -7.8169,  -7.1867,  -7.4152,  -9.8344,\n",
       "          -3.6974,  -6.6184,  -8.4081,  -6.7142,  -9.1387,  -2.6419,  -1.9076,\n",
       "          -6.2831,  -9.3299,  -8.0185,  -8.5933, -11.9963, -11.7179,  -8.1385,\n",
       "         -12.3799, -11.0510,  -5.5329,   5.2269,  -9.9811,  -7.4056, -10.3953,\n",
       "         -10.0786,  -3.6249,  -8.9944,  -6.3132,  -5.4102,  -5.9791,  -7.8561]],\n",
       "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_res = model(**res)\n",
    "model_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-chosen",
   "metadata": {},
   "source": [
    "decode the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "decimal-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 5\n",
    "ps = torch.nn.functional.softmax(model_res.logits, dim=1)\n",
    "probs, classes = torch.topk(ps, topk, dim=1)\n",
    "probs = probs.tolist()\n",
    "classes = classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "removed-mother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.7232240438461304,\n",
       "   0.25969767570495605,\n",
       "   0.006111669819802046,\n",
       "   0.0060967495664954185,\n",
       "   0.0011588569032028317,\n",
       "   0.000731807027477771,\n",
       "   0.0005764953093603253,\n",
       "   0.0004914401797577739,\n",
       "   0.0002766131074167788,\n",
       "   0.00024830058100633323,\n",
       "   0.00020814224262721837,\n",
       "   0.00019130650616716594,\n",
       "   0.00018323035328648984,\n",
       "   0.00010350668162573129,\n",
       "   0.00010080193169414997,\n",
       "   9.626983955968171e-05,\n",
       "   5.7873698096955195e-05,\n",
       "   5.095693632028997e-05,\n",
       "   2.9587761673610657e-05,\n",
       "   2.1597934392048046e-05]],\n",
       " [[101,\n",
       "   42,\n",
       "   23,\n",
       "   50,\n",
       "   30,\n",
       "   15,\n",
       "   90,\n",
       "   62,\n",
       "   89,\n",
       "   8,\n",
       "   49,\n",
       "   73,\n",
       "   28,\n",
       "   106,\n",
       "   26,\n",
       "   84,\n",
       "   39,\n",
       "   68,\n",
       "   66,\n",
       "   40]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-trigger",
   "metadata": {},
   "source": [
    "helper method from torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dimensional-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TEA': 0.7232240438461304,\n",
       "  'GROCERY': 0.25969767570495605,\n",
       "  'COFFEE': 0.006111669819802046,\n",
       "  'HERB': 0.0060967495664954185,\n",
       "  'EDIBLE_OIL_VEGETABLE': 0.0011588569032028317,\n",
       "  'BREAD': 0.000731807027477771,\n",
       "  'SKIN_MOISTURIZER': 0.0005764953093603253,\n",
       "  'LEGUME': 0.0004914401797577739,\n",
       "  'SKIN_CLEANING_AGENT': 0.0002766131074167788,\n",
       "  'BEAUTY': 0.00024830058100633323,\n",
       "  'HEALTH_PERSONAL_CARE': 0.00020814224262721837,\n",
       "  'PANTRY': 0.00019130650616716594,\n",
       "  'DRINKING_CUP': 0.00018323035328648984,\n",
       "  'VITAMIN': 0.00010350668162573129,\n",
       "  'DAIRY_BASED_DRINK': 0.00010080193169414997,\n",
       "  'SAUCE': 9.626983955968171e-05,\n",
       "  'FOOD_SERVICE_SUPPLY': 5.7873698096955195e-05,\n",
       "  'NUTRITIONAL_SUPPLEMENT': 5.095693632028997e-05,\n",
       "  'MEDICATION': 2.9587761673610657e-05,\n",
       "  'FURNITURE': 2.1597934392048046e-05}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_class_to_label(probs, id2label_str, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-inquiry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "naughty-killer",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "julian-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../artifacts/dataset_processed/amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "closed-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = datasets.load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ambient-feedback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brand', 'item_id', 'item_name', 'main_image_id', 'node'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = set(raw_datasets['test'].column_names ) - set(['text','label'])\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "scientific-worcester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test', 'train', 'valid'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(raw_datasets.column_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-entrance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-sussex",
   "metadata": {},
   "source": [
    "tokenize the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "executed-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec430bd96e42989c27c16daa900b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../artifacts/dataset_processed/amazon/test/cache-07925e711c10b453.arrow\n",
      "Loading cached processed dataset at ../artifacts/dataset_processed/amazon/valid/cache-07925e711c10b453.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "supported-belarus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention_mask', 'input_ids', 'label', 'text'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['test'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of input length of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unusual-reflection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len ( tokenized_datasets['test'][0]['input_ids'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-dressing",
   "metadata": {},
   "source": [
    "create a subset of the test dataset.\n",
    "feel free to use the full dataset if running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "premium-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = tokenized_datasets[\"test\"].num_rows\n",
    "subset = 1_000\n",
    "\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(42).select(range(subset)) \n",
    "test_dataset.set_format(type='torch' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "linear-musical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-monte",
   "metadata": {},
   "source": [
    "## Predicting on Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-recognition",
   "metadata": {},
   "source": [
    "Predicting using the hugging face trainer object.     \n",
    "\n",
    "Optimized Batch inference workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "operational-newton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define test trainer\n",
    "test_trainer = transformers.Trainer(model) \n",
    "# Make prediction\n",
    "raw_predictions, raw_label_ids, raw_metrics = test_trainer.predict(test_dataset) \n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "royal-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_accuracy = datasets.load_metric('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "vietnamese-stockholm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.918}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_accuracy.compute(predictions = y_pred, references = test_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "composed-disposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-paste",
   "metadata": {},
   "source": [
    "Predicting using the model `__call__` dunder method\n",
    "\n",
    "need to make sure batches can fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "velvet-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ -5.0694,  -5.1765,  -8.7472,  -0.8668,  -5.7447,  -6.1302,  -7.2811,\n",
       "          -8.6489,  -5.1904,  -4.6450,  -4.6303,  -2.8763,  -6.6729,  -4.6171,\n",
       "          -6.1057,  -6.2088,  -5.0244,  11.6435,  -4.6505,  -6.0849,  -9.2568,\n",
       "          -4.3728,  -8.5708,  -5.4817,  -4.7679,  -5.2058,  -9.7425,  -4.9433,\n",
       "          -6.2621,  -7.7707,  -6.8868,  -9.0698,  -9.1490,  -6.0963,  -5.7131,\n",
       "          -7.2520,  -7.7580,  -6.5021,  -4.8559, -10.4029,  -2.2170,  -4.1551,\n",
       "          -4.8708,  -6.6222,  -8.4910,  -8.9385, -10.5362,  -9.2823,  -4.3964,\n",
       "          -4.1102, -10.8039,  -2.9993,  -2.7675,  -3.8705,  -7.8226,  -4.2509,\n",
       "          -8.1575, -11.0981,  -6.9510,  -6.2460,  -9.7539, -10.3530, -10.3915,\n",
       "          -8.3798,  -8.6267,  -6.7258,  -5.2902,  -5.5456,  -7.4572,  -6.7893,\n",
       "          -6.6350,  -5.5757,  -6.1036,  -8.0627,  -5.0114,  -5.9468,  -7.6404,\n",
       "          -5.8532,  -4.6515,  -8.7958,  -2.2132,  -5.7307, -11.2271,  -8.5976,\n",
       "          -6.0319,  -5.3360,  -3.4965,  -6.6903,  -3.4729,  -4.7851,  -5.1181,\n",
       "          -5.1694,  -7.3753,  -8.2151,  -5.3244,  -8.1673,  -9.1674,  -5.4894,\n",
       "          -7.8988,  -7.8357,  -6.9620,  -7.4507,  -8.0417,  -8.0450,  -7.2707,\n",
       "          -5.5206,  -6.1854,  -6.7445,  -5.2833,  -6.0423,  -1.0131,  -7.7638],\n",
       "        [ -4.9397,  -5.0942,  -8.9183,  -0.8131,  -5.6316,  -6.1482,  -7.0770,\n",
       "          -8.5823,  -5.3128,  -4.6842,  -4.6181,  -2.9536,  -6.6262,  -4.8476,\n",
       "          -6.1653,  -6.5085,  -5.0320,  11.6988,  -4.6767,  -6.0004,  -9.2643,\n",
       "          -4.4226,  -8.5317,  -5.6090,  -4.7091,  -5.1683, -10.0103,  -4.9782,\n",
       "          -6.2352,  -7.9545,  -7.0065,  -9.0330,  -9.1156,  -6.3067,  -5.8882,\n",
       "          -7.3933,  -8.0445,  -6.4756,  -4.7440, -10.4396,  -2.2014,  -4.2164,\n",
       "          -5.0912,  -6.5722,  -8.5177,  -8.9027, -10.6509,  -9.3336,  -4.3356,\n",
       "          -4.1572, -10.9908,  -2.9825,  -2.6097,  -3.8557,  -7.8364,  -4.4348,\n",
       "          -8.0256, -11.1113,  -6.8093,  -6.1334,  -9.6631, -10.3698, -10.5581,\n",
       "          -8.3804,  -8.7581,  -6.6761,  -5.4132,  -5.7523,  -7.5883,  -6.7698,\n",
       "          -6.5214,  -5.7072,  -6.0151,  -8.0841,  -4.9543,  -6.1174,  -7.7501,\n",
       "          -5.6729,  -4.4189,  -8.6772,  -2.4891,  -5.8684, -11.2026,  -8.8104,\n",
       "          -6.2939,  -5.2641,  -3.4292,  -6.6928,  -3.7206,  -4.9089,  -5.3068,\n",
       "          -5.3333,  -7.2503,  -8.0520,  -5.3576,  -8.0527,  -9.1814,  -5.5296,\n",
       "          -7.7832,  -7.7561,  -7.0091,  -7.5555,  -7.9874,  -7.8868,  -7.0989,\n",
       "          -5.3529,  -6.4130,  -6.8589,  -5.5285,  -6.0788,  -1.0674,  -7.7326],\n",
       "        [ -7.1063,  -8.1591,  -8.8667,  -4.6394,  -7.3704, -10.5091,  -9.7503,\n",
       "          -7.6486,  -9.7326,  -9.6218, -10.8994,  -9.9278,  -3.4800, -13.8761,\n",
       "          -9.7811, -15.8806,  -8.3492,  -7.9783,  -6.9030,  -7.6508, -10.6418,\n",
       "         -10.1883,  -7.1844, -10.3427,  -3.9261,  -4.7283, -17.6463,  -6.7690,\n",
       "          -3.5812, -11.0206, -13.5304,  -5.8222,  -4.7379,  -9.8681,  -9.9482,\n",
       "         -10.5597,  -9.3162,  -7.0803,  -9.5422,  -5.3222,  -9.7088,  -6.2462,\n",
       "          -8.4668,  -8.3909,  -3.8106,  -4.8857, -10.6460, -13.4181,  -4.3052,\n",
       "          -9.2862, -14.3417,  -3.2700,  -7.4718,  -5.6745,  -5.5540, -10.1412,\n",
       "          -3.4693,  -6.3539, -13.1361,  -4.9557,  -2.8736,  -9.4281, -15.8416,\n",
       "          -8.0546,  -5.1598,  -8.2937, -12.7350, -11.3986, -13.0108,  -2.2599,\n",
       "           1.0568, -11.1272,  -6.2840,  -9.2201,  -6.5314, -12.8883,  -7.2812,\n",
       "          -9.0668,  -5.6520, -10.4130,  -9.0599,  -9.1490,  -6.5379, -13.3803,\n",
       "         -18.2669,  -8.5140,  -5.1044,  -8.7081, -14.4484, -12.1920, -11.3911,\n",
       "         -10.7906,  -4.9194,  -9.5114,  -9.9811,  -9.0049,  -4.2193,  -6.3393,\n",
       "          -9.7814,  -9.1595,  -8.2039,  -9.8136,  -8.7766,  -7.2595,  -8.4967,\n",
       "          -9.7294, -12.4050,  -8.0445, -10.1631,  -7.9738,  -4.0293,  -2.8760],\n",
       "        [ -6.2296,  -5.4458,  -2.3414,  -4.4724,  -6.1175,  -2.1800,  -4.4036,\n",
       "          -8.2107,  -3.8357,  -5.1197,  -5.6704,  -5.0077,  -8.3744,   0.8957,\n",
       "          -6.0175,  -4.4053,  -4.4428,  -5.1305,  -4.7252,  -7.9307,  -8.4962,\n",
       "          -3.6694,  -6.7461,  -4.7714,  -4.9277,  -4.1692,  -7.6111,  -4.8102,\n",
       "          -7.9050,  -5.0130,  -7.8269,  -7.1033,  -9.1273,  -3.4614,  -4.2537,\n",
       "          -5.1410,  -6.4806,  -4.4321,  -9.0485, -13.4424,  -5.1450,  -5.0209,\n",
       "          -2.8123,  -1.8150,  -8.3212,  -9.3899,  -3.3616,  -5.0398,  -5.8116,\n",
       "          -3.4433,  -6.7552,  -3.0887,  -3.1609,  -3.6616,  -6.3152,  -3.0968,\n",
       "          -7.9745,  -9.0549,  -8.9565,  -8.2429, -12.3327,  -7.3725,  -6.8476,\n",
       "          -8.7636,  -7.3671,  -5.1388,  -5.4978,  -4.6900,  -6.1551,  -7.4574,\n",
       "          -7.0144,  -4.9010,  -5.9283,  -4.8728,  -4.9348,  -4.9332,  -6.2274,\n",
       "          -7.6572,  -7.1067,  -7.0950,  -4.5889,  -6.4235, -10.2985,   0.9251,\n",
       "          -8.1813,  -9.8154,  -5.6585,  -6.0569,   9.7156,  -4.4282,  -5.6042,\n",
       "          -4.0920,  -7.1514,  -3.4706,  -2.7239,  -6.3123, -10.1675,  -6.1813,\n",
       "          -8.5268,  -6.7475,  -5.7250,  -5.9554,  -6.4873,  -7.0126, -10.7098,\n",
       "          -5.8327,  -6.2294,  -5.8272,  -4.7996,  -7.1404,  -6.0473,  -6.5459],\n",
       "        [ -5.9030,  -5.2533,  -5.8680,  -5.4198,  -3.5442,  -8.4558,  -6.2790,\n",
       "          -3.1124,  -6.3494,  -5.1496,  -4.3549,  -5.9909,  -5.5450,  -8.6844,\n",
       "          -8.4433,  -8.6998,  -4.8355,  -5.7072,  -3.4889,  -7.0916,  -2.7878,\n",
       "          -5.3074,  -5.8094,  -6.1466,  -4.2281,  -4.6936, -11.4308,  -6.3137,\n",
       "          -4.8532, -10.9322,  -5.4241,  -4.3067,  -7.0799,  -8.4372,  -8.0759,\n",
       "          -9.6559,  -7.3893,  -2.7155,  -4.1153,  -6.9866,  -6.1615,  -7.4015,\n",
       "          -3.9424,  -5.2015,  -5.9776,  -1.9724,  -4.9721,  -6.1864,  -4.8613,\n",
       "          -3.8765,  -6.4651,  -3.2226,  -2.6753,  -1.9201,   1.7363,  -6.6608,\n",
       "          -3.6293,  -1.4194,  -8.4685,  -4.4365,  -8.0426,  -4.9659,  -7.8825,\n",
       "           6.5514,  -4.7890,  -7.4027,  -7.2244,  -8.6447,  -8.2846,  -4.2094,\n",
       "          -4.9573,  -5.2829,  -3.6212,  -5.9972,  -5.3791,  -6.4178,  -5.1385,\n",
       "          -1.2332,  -6.5486,  -7.1846,  -6.2625,  -4.6902,  -3.7249,  -6.5311,\n",
       "         -10.5522,  -4.3230,  -7.1062,  -5.0882,  -7.2710,  -5.7199,  -7.0763,\n",
       "          -3.4961,  -1.8972,  -5.4785,  -6.9594,  -9.8590,  -7.3061,  -6.7407,\n",
       "          -6.5238,  -7.9038,  -6.7370,  -5.8330,  -7.1346,  -6.9216,  -6.7194,\n",
       "          -6.8426,  -8.5847,  -8.8641,  -8.6039,  -4.4359,  -4.2170,  -4.9517]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model( input_ids = test_dataset['input_ids'][0:5].to(device) \n",
    "      , attention_mask = test_dataset['attention_mask'][0:5].to(device) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-segment",
   "metadata": {},
   "source": [
    "helper method that we can use to compute accuracy on our full dataset. \n",
    "\n",
    "We can't use HF Trainer object , because it is hard to use with the torcshcript model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "documented-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_batch(model, dataset, device = device, batch_size = 8):\n",
    "    l = len(dataset)\n",
    "    all_y_preds = []\n",
    "    # make sure model is in eval mode ; not computing gradients\n",
    "    model.eval()\n",
    "    \n",
    "    # feed model to cpu/gpu device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # iterate our dataset in batches\n",
    "    for ndx in trange(0, l, batch_size):\n",
    "        \n",
    "        # take precomputed inut and attention masks\n",
    "        input_ids = dataset['input_ids'][ndx:ndx+batch_size].to(device) \n",
    "        attention_mask = dataset['attention_mask'][ndx:ndx+batch_size].to(device) \n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            res = model( input_ids = input_ids, attention_mask = attention_mask )\n",
    "            \n",
    "            # output of torchscript model doesn't have logits property \n",
    "            #logits = res.logits.detach().cpu().numpy()\n",
    "            \n",
    "            logits = res[0].detach().cpu().numpy()\n",
    "            \n",
    "            y_preds = np.argmax(logits, axis=1)\n",
    "            \n",
    "            all_y_preds.extend(y_preds)\n",
    "    \n",
    "    # accuracy on whole dataset\n",
    "    accuracy = metric_accuracy.compute(predictions = all_y_preds, references = dataset['label'])\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "vanilla-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:49<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.918}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_batch(model,test_dataset,device='cpu' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-newport",
   "metadata": {},
   "source": [
    "## Optimization: Quantizing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-cooperative",
   "metadata": {},
   "source": [
    "Pytorch supports three types of Quantization:\n",
    "1. Dynamic Qunatization\n",
    "2. Static Quantization\n",
    "3. Qunatization Aware Training \n",
    "\n",
    "In this notebook, we look at Dyanmic Quantization.\n",
    "\n",
    "**Dynamic Qunatization**, quantizes the weights . The activations are quantized on the fly.\n",
    "\n",
    "Currently Pytorch doesn't support dynamic quantization on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "scheduled-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=112, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-appliance",
   "metadata": {},
   "source": [
    "In the below code, we quantize all the `Linear` layers to int8 from float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "likely-looking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=112, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "quantized_model_int8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "How much file storage did we gain ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "floral-collins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 268.197041\n",
      "Size (MB): 268.225625\n",
      "Size (MB): 138.796889\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "The quantized model reduced the file storage by half, which also translates to less memory and faster execution ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-switch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opponent-session",
   "metadata": {},
   "source": [
    "### Benchmark Qunatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-making",
   "metadata": {},
   "source": [
    "Lets measure how much speedup and possible loss in accuracy , we gained from quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "leading-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_model_evaluation(model, dataset=test_dataset,device:str='cpu'):\n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    result = prediction_batch(model , dataset, device=device )\n",
    "    \n",
    "    eval_end_time = time.time()\n",
    "    eval_duration_time = eval_end_time - eval_start_time\n",
    "    print(result)\n",
    "    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-logistics",
   "metadata": {},
   "source": [
    "Evaluate the original FP32 BERT model on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "threaded-prior",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:16<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.918}\n",
      "Evaluate total time (seconds): 16.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(model, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-display",
   "metadata": {},
   "source": [
    "Evaluate the original FP32 BERT model on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "speaking-profile",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:49<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.918}\n",
      "Evaluate total time (seconds): 109.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(model, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-offering",
   "metadata": {},
   "source": [
    "Evaluate the qunatized int8 BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "certified-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:32<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.919}\n",
      "Evaluate total time (seconds): 92.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_model_evaluation(quantized_model_int8, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-reggae",
   "metadata": {},
   "source": [
    "dynamtic int8 qunatization is not supported on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expensive-maximum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCould not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. \\nThis could be because the operator doesn't exist for this backend, or was omitted during \\nthe selective/custom build process (if using custom build). \\nIf you are a Facebook employee using PyTorch on mobile, \\nplease visit https://fburl.com/ptmfixes for possible resolutions. \\n'quantized::linear_dynamic' is only available for these backends: [CPU, BackendSelect, Named, \\nADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, \\nAutogradMLC, Tracer, Autocast, Batched, VmapMode].\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time_model_evaluation(quantized_model_int8, device='cuda')\n",
    "\"\"\"\n",
    "Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. \n",
    "This could be because the operator doesn't exist for this backend, or was omitted during \n",
    "the selective/custom build process (if using custom build). \n",
    "If you are a Facebook employee using PyTorch on mobile, \n",
    "please visit https://fburl.com/ptmfixes for possible resolutions. \n",
    "'quantized::linear_dynamic' is only available for these backends: [CPU, BackendSelect, Named, \n",
    "ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, \n",
    "AutogradMLC, Tracer, Autocast, Batched, VmapMode].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-spencer",
   "metadata": {},
   "source": [
    "| Model | device | Accuracy | Time |\n",
    "| --- | --- | --- |--- |\n",
    "| original fp32 | cpu | .918 | 109.3 |\n",
    "| original fp32 | gpu | .918 | 16.6 |\n",
    "| quantized int8 | cpu | .919 | 92.8 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-beverage",
   "metadata": {},
   "source": [
    "the above results , were measured in the below config.  \n",
    "You might need to reduce the OpenMP threads, if introducing parralelism in your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "endless-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATen/Parallel:\n",
      "\tat::get_num_threads() : 8\n",
      "\tat::get_num_interop_threads() : 8\n",
      "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "\tomp_get_max_threads() : 8\n",
      "Intel(R) oneAPI Math Kernel Library Version 2021.3-Product Build 20210617 for Intel(R) 64 architecture applications\n",
      "\tmkl_get_max_threads() : 8\n",
      "Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n",
      "std::thread::hardware_concurrency() : 16\n",
      "Environment variables:\n",
      "\tOMP_NUM_THREADS : [not set]\n",
      "\tMKL_NUM_THREADS : [not set]\n",
      "ATen parallel backend: OpenMP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ( torch.__config__.parallel_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-replication",
   "metadata": {},
   "source": [
    "## Optimization: TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-beast",
   "metadata": {},
   "source": [
    "TorchScript is a way to create serializable and optimizable models from PyTorch code\n",
    "\n",
    "load model as torchscript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-learning",
   "metadata": {},
   "source": [
    "According to [Hugging Face docs](https://huggingface.co/transformers/torchscript.html), the `torchscript` flag\n",
    "\n",
    ">This flag is necessary because most of the language models in this repository have tied weights between their Embedding layer and their Decoding layer. \n",
    ">\n",
    ">TorchScript does not allow the export of models that have tied weights, it is therefore necessary to untie the weights beforehand.\n",
    ">\n",
    ">This implies that models instantiated with the torchscript flag have their Embedding layer and Decoding layer separate, which means that they should not be trained down the line. Training would de-synchronize the two layers, leading to unexpected results.\n",
    ">\n",
    ">This is not the case for models that do not have a Language Model head, as those do not have tied weights. \n",
    "These models can be safely exported without the torchscript flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir, torchscript=True)\n",
    "script_tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir , torchscript=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-strategy",
   "metadata": {},
   "source": [
    "create a dummy input to pass to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = query\n",
    "res = script_tokenizer.encode_plus(text, return_tensors=\"pt\", padding=\"max_length\",truncation=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = res['input_ids']\n",
    "masks_tensors = res['attention_mask']\n",
    "\n",
    "dummy_input = [tokens_tensor, masks_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-liquid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hindu-pricing",
   "metadata": {},
   "source": [
    "Creating the trace by passing a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "script_model = script_model.to(device)\n",
    "\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "masks_tensors = masks_tensors.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(script_model, [tokens_tensor, masks_tensors])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-hebrew",
   "metadata": {},
   "source": [
    "prediction using traced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = traced_model(tokens_tensor, segments_tensors)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list ( zip (labels , torch.softmax(res[0], dim=1).tolist()[0] ) )\n",
    "predictions = sorted (predictions , key=lambda x:x[1] , reverse =True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-corpus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "micro-scheme",
   "metadata": {},
   "source": [
    "lets compute how long it takes to predict with traced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_model_evaluation(traced_model, device='cpu')\n",
    "#prediction_batch(traced_model,test_dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_model_evaluation(traced_model, device='gpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-bennett",
   "metadata": {},
   "source": [
    "update below table\n",
    "| Model | device | Accuracy | Time |\n",
    "| --- | --- | --- |--- |\n",
    "| original fp32 | cpu | .918 | 109.3 |\n",
    "| original fp32 | gpu | .918 | 16.6 |\n",
    "| quantized int8 | cpu | .919 | 92.8 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-pension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-conservative",
   "metadata": {},
   "source": [
    "Save / load  traced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_trace ='../artifacts/model/amazon_trace/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(traced_model, f\"{model_dir_trace}/traced_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-accuracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.jit.load( f\"{model_dir_trace}/traced_model.pt\")\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-summary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-armor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-gibson",
   "metadata": {},
   "source": [
    "## Save Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-monitor",
   "metadata": {},
   "source": [
    "For the next section, let us save the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir}index_to_name.json','w') as f:\n",
    "    json.dump(id2label_str,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir_trace}index_to_name.json','w') as f:\n",
    "    json.dump(id2label_str,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-composition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "civic-zimbabwe",
   "metadata": {},
   "source": [
    "in a separate json config, let use store things like \n",
    "- was lower case used by the tokenizer\n",
    "- max length of tokenizer\n",
    "- num labels\n",
    "- is the model a jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_dir_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_config = {\n",
    " \"model_name\":\"pt-original\",\n",
    " \"do_lower_case\": tokenizer.do_lower_case,\n",
    " \"num_labels\":len(id2label),\n",
    " \"save_mode\":\"original\",\n",
    " \"max_length\":tokenizer.model_max_length,\n",
    " \"captum_explanation\": True,\n",
    " \"embedding_name\": \"bert\",\n",
    " \"top_k\": 5   \n",
    "\n",
    "}\n",
    "\n",
    "setup_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir}setup_config.json','w') as f:\n",
    "    json.dump(setup_config,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_config_trace = {**setup_config}\n",
    "setup_config_trace['model_name'] = \"pt-jit\"\n",
    "setup_config_trace['captum_explanation'] = False\n",
    "setup_config_trace['save_mode'] = 'jit'\n",
    "setup_config_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_dir_trace}setup_config.json','w') as f:\n",
    "    json.dump(setup_config_trace,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-catalyst",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
